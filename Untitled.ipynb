{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cosmetic-simpson",
   "metadata": {},
   "source": [
    "freeze(self, q_in, q_out)# 1、后量化训练\n",
    "## 1.1 线性量化\n",
    "https://zhuanlan.zhihu.com/p/156835141\n",
    "\n",
    "scale:\n",
    "> s = rmax - rmin / qmax - qmin\n",
    "\n",
    "zero point:\n",
    "> z = round(qmax - rmax / s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-comparison",
   "metadata": {},
   "source": [
    "### 1. 量化基本公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dense-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exciting-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 scale 和 zero_point 的基本公式\n",
    "def calcu_scale_and_zeropoint(min_val, max_val, num_bits=8):\n",
    "    q_min = 0.\n",
    "    q_max = 2. ** num_bits - 1\n",
    "    scale = float((max_val - min_val) / (q_max - q_min))\n",
    "    zero_point = np.clip(int(q_max - max_val / scale), q_min, q_max)\n",
    "    \n",
    "    return scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "northern-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor 量化和反量化\n",
    "def quantize_tensor(x, scale, zero_point, num_bits=8, signed=False):\n",
    "    if signed:\n",
    "        q_min = - 2. ** (num_bits - 1)\n",
    "        q_max = 2. ** (num_bits - 1) - 1.\n",
    "    else:\n",
    "        q_min = 0.\n",
    "        q_max = 2. ** num_bits -1.\n",
    "        \n",
    "    q_x = x / scale + zero_point\n",
    "    q_x.clamp_(q_min, q_max).round()  # q=round(r/S+Z)\n",
    "    \n",
    "    return q_x.float()   # 由于pytorch不支持int类型的运算，因此我们还是用float来表示整数\n",
    "\n",
    "def dequantize_tensor(q_x, scale, zero_point):\n",
    "    return scale * (q_x -zero_point)  # r=S(q-Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-conversation",
   "metadata": {},
   "source": [
    "### 2. 封装成类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "palestinian-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QParam:\n",
    "    def __init__(self, num_bits=8):\n",
    "        self.num_bits = num_bits\n",
    "        self.scale = None\n",
    "        self.zero_point = None\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "    \n",
    "    def update(self, tensor):\n",
    "        self.max = max(0, tensor.max()) if self.max is None \\\n",
    "                                        else max(0, self.max, tensor.max())\n",
    "        self.min = min(0, tensor.min()) if self.min is None \\\n",
    "                                        else min(0, self.min, tensor.min())\n",
    "        self.scale, self.zero_point = calcu_scale_and_zeropoint(self.min, self.max, self.num_bits)\n",
    "        \n",
    "    def quantize_tensor(self, tensor):\n",
    "        return quantize_tensor(tensor, self.scale, self.zero_point, num_bits=self.num_bits)\n",
    "    \n",
    "    def dequantize_tensor(self, q_x):\n",
    "        return dequantize_tensor(q_x, self.scale, self.zero_point)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-bernard",
   "metadata": {},
   "source": [
    "### 3、量化网络基类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alleged-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModule(nn.Module):\n",
    "    def __init__(self, has_qin=True, has_qout=True, num_bits=8):\n",
    "        # 指定量化的位数外，还需指定是否提供量化输入 (qin) 及输出参数 (qout)\n",
    "        # 不是每一个网络模块都需要统计输入的 min、max，大部分中间层都是用上一层的 qout 来作为自己的 qin 的，\n",
    "        # 另外有些中间层的激活函数也是直接用上一层的 qin 来作为自己的 qin 和 qout。\n",
    "        super(QModule, self).__init__()\n",
    "        if has_qin:\n",
    "            self.q_in = QParam(num_bits)\n",
    "        if has_qout:\n",
    "            self.q_out = QParam(num_bits)\n",
    "    \n",
    "    def freeze(self):\n",
    "        # 函数会在统计完 min、max 后发挥作用\n",
    "        # 很多项是可以提前计算好的，freeze 就是把这些项提前固定下来\n",
    "        # 同时也将网络的权重由浮点实数转化为定点整数。\n",
    "        pass\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        # 在量化 inference 的时候会使用\n",
    "        raise NotImplementedError('quantize_inference should be implemented.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-chaos",
   "metadata": {},
   "source": [
    "### 4、量化卷积层类的定义\n",
    "<img src=\"https://www.zhihu.com/equation?tex=a%3D%5Csum_%7Bi%7D%5EN+w_i+x_i%2Bb+%5Ctag%7B1%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />\n",
    "由此得到量化的公式\n",
    "<img src=\"https://www.zhihu.com/equation?tex=S_a+%28q_a-Z_a%29%3D%5Csum_%7Bi%7D%5EN+S_w%28q_w-Z_w%29S_x%28q_x-Z_x%29%2BS_b%28q_b-Z_b%29+%5Ctag%7B2%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />\n",
    "<img src=\"https://www.zhihu.com/equation?tex=q_a%3D%5Cfrac%7BS_w+S_x%7D%7BS_a%7D%5Csum_%7Bi%7D%5EN+%28q_w-Z_w%29%28q_x-Z_x%29%2B%5Cfrac%7BS_b%7D%7BS_a%7D%28q_b-Z_b%29%2BZ_a+%5Ctag%7B3%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />\n",
    "\n",
    "\n",
    "> <img src=\"./image.png\" style=\"zoom:60%;\" />\n",
    "\n",
    "经过调整：\n",
    "\n",
    "<img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+q_a%26%3D%5Cfrac%7BS_w+S_x%7D%7BS_a%7D%28%5Csum_%7Bi%7D%5EN%28q_w-Z_w%29%28q_x-Z_x%29%2Bq_b%29%2BZ_a+%5Cnotag+%5C%5C+%26%3DM%28%5Csum_%7Bi%7D%5EN+q_wq_x-%5Csum_i%5EN+q_wZ_x-%5Csum_i%5EN+q_xZ_w%2B%5Csum_i%5ENZ_wZ_x%2Bq_b%29%2BZ_a+%5Ctag%7B4%7D+%5Cend%7Balign%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "immediate-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv2d(QModule):\n",
    "    def __init__(self, conv_module, has_qin=True, has_qout=True, num_bits=8):\n",
    "        super(QConv2d, self).__init__(has_qin, has_qout, num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.conv_module = conv_module\n",
    "        self.q_weight = QParam(num_bits=num_bits)\n",
    "    \n",
    "    def freeze(self, q_in=None, q_out=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'q_out') and q_out is not None:\n",
    "            raise ValueError('q_out has been provided in init function.')\n",
    "        if not hasattr(self, 'q_out') and q_out is None:\n",
    "            raise ValueError('q_out is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "        if q_out is not None:\n",
    "            self.q_out = q_out\n",
    "        \n",
    "        # 计算 M = s_w * s_in / s_out\n",
    "        self.M = self.q_weight.scale * self.q_in.scale / self.q_out.scale\n",
    "        \n",
    "        # 量化卷积层中的权重\n",
    "        self.conv_module.weight.data = self.q_weight.quantize_tensor(self.conv_module.weight.data) \\\n",
    "                                        - self.q_weight.zero_point\n",
    "        # 量化卷积层中的偏置\n",
    "        self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data, scale=self.q_in.scale * self.q_weight.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            \n",
    "        self.q_weight.update(self.conv_module.weight.data)\n",
    "        \n",
    "        # 伪量化节点\n",
    "        self.conv_module.weight.data = self.q_weight.quantize_tensor(self.conv_module.weight.data)\n",
    "        self.conv_module.weight.data = self.q_weight.dequantize_tensor(self.conv_module.weight.data)\n",
    "        \n",
    "        x = self.conv_module(x)\n",
    "        \n",
    "        if hasattr(self, 'q_out'):\n",
    "            self.q_out.update(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.q_in.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_()\n",
    "        x = x + self.q_out.zero_point\n",
    "        x.clamp_(0., 2. ** self.num_bits - 1.).round_()\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-treat",
   "metadata": {},
   "source": [
    "### 5、量化线性层类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stretch-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinear(QModule):\n",
    "    def __init__(self, fc_module, has_qin=True, has_qout=True, num_bits=8):\n",
    "        super(QLinear, self).__init__(has_qin, has_qout, num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.fc_module = fc_module\n",
    "        self.q_weight = QParam(num_bits=num_bits)\n",
    "    \n",
    "    def freeze(self, q_in=None, q_out=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'q_out') and q_out is not None:\n",
    "            raise ValueError('q_out has been provided in init function.')\n",
    "        if not hasattr(self, 'q_out') and q_out is None:\n",
    "            raise ValueError('q_out is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "        if q_out is not None:\n",
    "            self.q_out = q_out\n",
    "        \n",
    "        # 计算 M = s_w * s_in / s_out\n",
    "        self.M = self.q_weight.scale * self.q_in.scale / self.q_out.scale\n",
    "        \n",
    "        # 量化卷积层中的权重\n",
    "        self.fc_module.weight.data = self.q_weight.quantize_tensor(self.fc_module.weight.data) \\\n",
    "                                        - self.q_weight.zero_point\n",
    "        # 量化卷积层中的偏置\n",
    "        self.fc_module.bias.data = quantize_tensor(self.fc_module.bias.data, scale=self.q_in.scale * self.q_weight.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            \n",
    "        self.q_weight.update(self.fc_module.weight.data)\n",
    "        \n",
    "        # 伪量化节点\n",
    "        self.fc_module.weight.data = self.q_weight.quantize_tensor(self.fc_module.weight.data)\n",
    "        self.fc_module.weight.data = self.q_weight.dequantize_tensor(self.fc_module.weight.data)\n",
    "        \n",
    "        x = self.fc_module(x)\n",
    "        \n",
    "        if hasattr(self, 'q_out'):\n",
    "            self.q_out.update(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.q_in.zero_point\n",
    "        x = self.fc_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_()\n",
    "        x = x + self.q_out.zero_point\n",
    "        x.clamp_(0., 2. ** self.num_bits - 1.).round_()\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-increase",
   "metadata": {},
   "source": [
    "### 6、量化ReLu 类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "revolutionary-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QReLU(QModule):\n",
    "    def __init__(self, has_qin=False, num_bits=None):\n",
    "        super(QReLU, self).__init__(has_qin=has_qin,num_bits=num_bits)\n",
    "        \n",
    "    def freeze(self, q_in=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        x = x.clone()\n",
    "        x[x < self.q_in.zero_point] = self.q_in.zero_point\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-default",
   "metadata": {},
   "source": [
    "### 7、量化最大池化层类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lonely-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMaxPooling2d(QModule):\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=0, has_qin=False, num_bits=None):\n",
    "        super(QMaxPooling2d, self).__init__(has_qin=has_qin, num_bits=num_bits)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "    def freeze(self, q_in=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            \n",
    "        x = F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        return F.max_pool2d(x, self.kernel_size, self.stride, self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "experimental-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 40, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(40, 40, 3, 1, groups=20)\n",
    "        self.fc = nn.Linear(5*5*40, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5*5*40)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def quantize(self, num_bits=8):\n",
    "        # 逐个量化每个模块\n",
    "        self.qconv1 = QConv2d(self.conv1, has_qin=True, has_qout=True, num_bits=num_bits)\n",
    "        self.qrelu1 = QReLU()\n",
    "        self.qmaxpool2d_1 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qconv2 = QConv2d(self.conv2, has_qin=False, has_qout=True, num_bits=num_bits)\n",
    "        self.qrelu2 = QReLU()\n",
    "        self.qmaxpool2d_2 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qfc = QLinear(self.fc, has_qin=False, has_qout=True, num_bits=num_bits)\n",
    "\n",
    "    def quantize_forward(self, x):\n",
    "        # 统计 min、max，同时模拟量化误差\n",
    "        x = self.qconv1(x)\n",
    "        x = self.qrelu1(x)\n",
    "        x = self.qmaxpool2d_1(x)\n",
    "        x = self.qconv2(x)\n",
    "        x = self.qrelu2(x)\n",
    "        x = self.qmaxpool2d_2(x)\n",
    "        x = x.view(-1, 5*5*40)\n",
    "        x = self.qfc(x)\n",
    "        return x\n",
    "\n",
    "    def freeze(self):\n",
    "        # 在统计 min、max后，对一些变量进行固化\n",
    "        self.qconv1.freeze()\n",
    "        self.qrelu1.freeze(self.qconv1.q_out)\n",
    "        self.qmaxpool2d_1.freeze(self.qconv1.q_out)\n",
    "        self.qconv2.freeze(q_in=self.qconv1.q_out)\n",
    "        self.qrelu2.freeze(self.qconv2.q_out)\n",
    "        self.qmaxpool2d_2.freeze(self.qconv2.q_out)\n",
    "        self.qfc.freeze(q_in=self.qconv2.q_out)\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        # 量化推理使用的函数\n",
    "        qx = self.qconv1.q_in.quantize_tensor(x)  # 输入量化到int8\n",
    "        qx = self.qconv1.quantize_inference(qx)\n",
    "        qx = self.qrelu1.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_1.quantize_inference(qx)\n",
    "        qx = self.qconv2.quantize_inference(qx)\n",
    "        qx = self.qrelu2.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_2.quantize_inference(qx)\n",
    "        qx = qx.view(-1, 5*5*40)\n",
    "        qx = self.qfc.quantize_inference(qx)\n",
    "        out = self.qfc.q_out.dequantize_tensor(qx)  # 输出反量化到 float\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "iraqi-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(NetBN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 40, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(40)\n",
    "        self.conv2 = nn.Conv2d(40, 40, 3, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(40)\n",
    "        self.fc = nn.Linear(5 * 5 * 40, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5 * 5 * 40)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def quantize(self, num_bits=8):\n",
    "        self.qconv1 = QConvBNReLU(self.conv1, self.bn1, has_qin=True, has_qout=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_1 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qconv2 = QConvBNReLU(self.conv2, self.bn2, has_qin=False, has_qout=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_2 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qfc = QLinear(self.fc, has_qin=False, has_qout=True, num_bits=num_bits)\n",
    "\n",
    "    def quantize_forward(self, x):\n",
    "        x = self.qconv1(x)\n",
    "        x = self.qmaxpool2d_1(x)\n",
    "        x = self.qconv2(x)\n",
    "        x = self.qmaxpool2d_2(x)\n",
    "        x = x.view(-1, 5*5*40)\n",
    "        x = self.qfc(x)\n",
    "        return x\n",
    "\n",
    "    def freeze(self):\n",
    "        self.qconv1.freeze()\n",
    "        self.qmaxpool2d_1.freeze(self.qconv1.q_out)\n",
    "        self.qconv2.freeze(q_in=self.qconv1.q_out)\n",
    "        self.qmaxpool2d_2.freeze(self.qconv2.q_out)\n",
    "        self.qfc.freeze(q_in=self.qconv2.q_out)\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        qx = self.qconv1.q_in.quantize_tensor(x)\n",
    "        qx = self.qconv1.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_1.quantize_inference(qx)\n",
    "        qx = self.qconv2.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_2.quantize_inference(qx)\n",
    "        qx = qx.view(-1, 5*5*40)\n",
    "\n",
    "        qx = self.qfc.quantize_inference(qx)\n",
    "        \n",
    "        out = self.qfc.q_out.dequantize_tensor(qx)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "approved-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quality-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (datas, targets) in enumerate(train_loader):\n",
    "        datas, targets = datas.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch:{} [{}/{}] \\t Loss: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(datas), len(train_loader.dataset), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "affiliated-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (datas, targets) in enumerate(test_loader):\n",
    "        datas, targets = datas.to(device), targets.to(device)\n",
    "        outputs = model(datas)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.0f}%\\n'.format(\n",
    "        test_loss, 100. * correct / len(test_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "diagnostic-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(batch_size, test_batch_size):\n",
    "    train_transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "    test_transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "    \n",
    "    train_dataset = datasets.MNIST(r'C:\\Users\\xia\\Documents\\datasets', train=True, download=True,\n",
    "                                  transform=train_transform)\n",
    "    test_dataset = datasets.MNIST(r'C:\\Users\\xia\\Documents\\datasets', train=False, download=True,\n",
    "                                 transform=test_transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=1)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n",
    "                                             shuffle=False, num_workers=1)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "thrown-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_quantize(model, test_loader):\n",
    "    for idx ,(datas, targets) in enumerate(test_loader,1):\n",
    "        output = model.quantize_forward(datas)\n",
    "        if idx % 500 == 0:\n",
    "            break\n",
    "    print('direct quantization finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "listed-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_inference(model, test_loader):\n",
    "    correct = 0\n",
    "    for idx, (datas, targets) in enumerate(test_loader, 1):\n",
    "        output = model(datas)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "    print('\\nTest set: Full Model Accuracy: {:.0f}%\\n'.format(100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "moderate-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_inference(model, test_loader):\n",
    "    correct = 0\n",
    "    for i, (datas, targets) in enumerate(test_loader, 1):\n",
    "        output = model.quantize_inference(datas)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "    print('\\nTest set: Quant Model Accuracy: {:.0f}%\\n'.format(100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "prescribed-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    batch_size = 64\n",
    "    test_batch_size = 64\n",
    "    using_bn = False\n",
    "    \n",
    "    train_loader, test_loader = dataset_loader(batch_size, test_batch_size)\n",
    "    \n",
    "    if using_bn:\n",
    "        model = NetBN()\n",
    "        model.load_state_dict(torch.load('ckpt/mnist_cnnbn.pt'))\n",
    "    else:\n",
    "        model = Net()\n",
    "        model.load_state_dict(torch.load('ckpt/mnist_cnn.pt'))\n",
    "    \n",
    "    model.eval()\n",
    "    full_inference(model, test_loader)\n",
    "\n",
    "    num_bits = 8\n",
    "    model.quantize(num_bits=num_bits)\n",
    "    model.eval()\n",
    "    print('Quantization bit: %d' % num_bits)\n",
    "\n",
    "    direct_quantize(model, train_loader)\n",
    "\n",
    "    model.freeze()\n",
    "\n",
    "    quantize_inference(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "younger-elephant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Full Model Accuracy: 98%\n",
      "\n",
      "Quantization bit: 8\n",
      "direct quantization finish\n",
      "\n",
      "Test set: Quant Model Accuracy: 98%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-candy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
