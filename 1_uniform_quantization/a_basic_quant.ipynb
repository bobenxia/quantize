{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "satellite-position",
   "metadata": {},
   "source": [
    "# 1、后量化训练\n",
    "## 1.1 线性量化\n",
    "https://zhuanlan.zhihu.com/p/156835141\n",
    "\n",
    "scale:\n",
    "> s = rmax - rmin / qmax - qmin\n",
    "\n",
    "zero point:\n",
    "> z = round(qmax - rmax / s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-fluid",
   "metadata": {},
   "source": [
    "### 1. 量化基本公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comfortable-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fourth-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 scale 和 zero_point 的基本公式\n",
    "def calcu_scale_and_zeropoint(min_val, max_val, num_bits=8):\n",
    "    q_min = 0.\n",
    "    q_max = 2. ** num_bits - 1\n",
    "    scale = float((max_val - min_val) / (q_max - q_min))\n",
    "    zero_point = np.clip(int(q_max - max_val / scale), q_min, q_max)\n",
    "    \n",
    "    return scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "banned-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor 量化和反量化\n",
    "def quantize_tensor(x, scale, zero_point, num_bits=8, signed=False):\n",
    "    if signed:\n",
    "        q_min = - 2. ** (num_bits - 1)\n",
    "        q_max = 2. ** (num_bits - 1) - 1.\n",
    "    else:\n",
    "        q_min = 0.\n",
    "        q_max = 2. ** num_bits -1.\n",
    "        \n",
    "    q_x = x / scale + zero_point\n",
    "    q_x.clamp_(q_min, q_max).round()  # q=round(r/S+Z)\n",
    "    \n",
    "    return q_x.float()   # 由于pytorch不支持int类型的运算，因此我们还是用float来表示整数\n",
    "\n",
    "def dequantize_tensor(q_x, scale, zero_point):\n",
    "    return scale * (q_x -zero_point)  # r=S(q-Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-faith",
   "metadata": {},
   "source": [
    "### 2. 封装成类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "neural-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QParam:\n",
    "    def __init__(self, num_bits=8):\n",
    "        self.num_bits = num_bits\n",
    "        self.scale = None\n",
    "        self.zero_point = None\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "    \n",
    "    def update(self, tensor):\n",
    "        self.max = max(0, tensor.max()) if self.max is None \\\n",
    "                                        else max(0, self.max, tensor.max())\n",
    "        self.min = min(0, tensor.min()) if self.min is None \\\n",
    "                                        else min(0, self.min, tensor.min())\n",
    "        self.scale, self.zero_point = calcu_scale_and_zeropoint(self.min, self.max, self.num_bits)\n",
    "        \n",
    "    def quantize_tensor(self, tensor):\n",
    "        return quantize_tensor(tensor, self.scale, self.zero_point, num_bits=self.num_bits)\n",
    "    \n",
    "    def dequantize_tensor(self, q_x):\n",
    "        return dequantize_tensor(q_x, self.scale, self.zero_point)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-glossary",
   "metadata": {},
   "source": [
    "### 3、量化网络基类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subtle-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModule(nn.Module):\n",
    "    def __init__(self, has_qin=True, has_qout=True, num_bits=8):\n",
    "        # 指定量化的位数外，还需指定是否提供量化输入 (qin) 及输出参数 (qout)\n",
    "        # 不是每一个网络模块都需要统计输入的 min、max，大部分中间层都是用上一层的 qout 来作为自己的 qin 的，\n",
    "        # 另外有些中间层的激活函数也是直接用上一层的 qin 来作为自己的 qin 和 qout。\n",
    "        super(QModule, self).__init__()\n",
    "        if has_qin:\n",
    "            self.q_in = QParam(num_bits)\n",
    "        if has_qout:\n",
    "            self.q_out = QParam(num_bits)\n",
    "    \n",
    "    def freeze(self):\n",
    "        # 函数会在统计完 min、max 后发挥作用\n",
    "        # 很多项是可以提前计算好的，freeze 就是把这些项提前固定下来\n",
    "        # 同时也将网络的权重由浮点实数转化为定点整数。\n",
    "        pass\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        # 在量化 inference 的时候会使用\n",
    "        raise NotImplementedError('quantize_inference should be implemented.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-nerve",
   "metadata": {},
   "source": [
    "### 4、伪量化定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceramic-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向推导中，先量化再反量化，引入噪声\n",
    "class FakeQuantize(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, qparam):\n",
    "        x = qparam.quantize_tensor(x)\n",
    "        x = qparam.dequantize_tensor(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None\n",
    "\n",
    "fake_quantize = FakeQuantize.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-underwear",
   "metadata": {},
   "source": [
    "### 4、量化卷积层类的定义\n",
    "<img src=\"https://www.zhihu.com/equation?tex=a%3D%5Csum_%7Bi%7D%5EN+w_i+x_i%2Bb+%5Ctag%7B1%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />\n",
    "由此得到量化的公式\n",
    "<img src=\"https://www.zhihu.com/equation?tex=S_a+%28q_a-Z_a%29%3D%5Csum_%7Bi%7D%5EN+S_w%28q_w-Z_w%29S_x%28q_x-Z_x%29%2BS_b%28q_b-Z_b%29+%5Ctag%7B2%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />\n",
    "<img src=\"https://www.zhihu.com/equation?tex=q_a%3D%5Cfrac%7BS_w+S_x%7D%7BS_a%7D%5Csum_%7Bi%7D%5EN+%28q_w-Z_w%29%28q_x-Z_x%29%2B%5Cfrac%7BS_b%7D%7BS_a%7D%28q_b-Z_b%29%2BZ_a+%5Ctag%7B3%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />\n",
    "\n",
    "\n",
    "> <img src=\"./image/image.png\" style=\"zoom:60%;\" />\n",
    "\n",
    "经过调整：\n",
    "\n",
    "<img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+q_a%26%3D%5Cfrac%7BS_w+S_x%7D%7BS_a%7D%28%5Csum_%7Bi%7D%5EN%28q_w-Z_w%29%28q_x-Z_x%29%2Bq_b%29%2BZ_a+%5Cnotag+%5C%5C+%26%3DM%28%5Csum_%7Bi%7D%5EN+q_wq_x-%5Csum_i%5EN+q_wZ_x-%5Csum_i%5EN+q_xZ_w%2B%5Csum_i%5ENZ_wZ_x%2Bq_b%29%2BZ_a+%5Ctag%7B4%7D+%5Cend%7Balign%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vocational-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv2d(QModule):\n",
    "    def __init__(self, conv_module, has_qin=True, has_qout=True, num_bits=8):\n",
    "        super(QConv2d, self).__init__(has_qin, has_qout, num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.conv_module = conv_module\n",
    "        self.q_weight = QParam(num_bits=num_bits)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            # 更新量化输入 q_in 的参数\n",
    "            self.q_in.update(x)\n",
    "            # 利用量化输入 q_in 伪量化全精度输入 x，引入噪声\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        self.q_weight.update(self.conv_module.weight.data)\n",
    "        x = F.conv2d(input = x, \n",
    "                     weight = fake_quantize(self.conv_module.weight, self.q_weight), \n",
    "                     bias = self.conv_module.bias, \n",
    "                     stride = self.conv_module.stride,\n",
    "                     padding = self.conv_module.padding, \n",
    "                     dilation=self.conv_module.dilation, \n",
    "                     groups = self.conv_module.groups)\n",
    "        \n",
    "        if hasattr(self, 'q_out'):\n",
    "            self.q_out.update(x)\n",
    "            x = fake_quantize(x, self.q_out)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def freeze(self, q_in=None, q_out=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'q_out') and q_out is not None:\n",
    "            raise ValueError('q_out has been provided in init function.')\n",
    "        if not hasattr(self, 'q_out') and q_out is None:\n",
    "            raise ValueError('q_out is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "        if q_out is not None:\n",
    "            self.q_out = q_out\n",
    "        \n",
    "        # 计算 M = s_w * s_in / s_out\n",
    "        self.M = self.q_weight.scale * self.q_in.scale / self.q_out.scale\n",
    "        \n",
    "        # 量化卷积层中的权重\n",
    "        self.conv_module.weight.data = self.q_weight.quantize_tensor(self.conv_module.weight.data) \\\n",
    "                                        - self.q_weight.zero_point\n",
    "        # 量化卷积层中的偏置\n",
    "        self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data, scale=self.q_in.scale * self.q_weight.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "        \n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.q_in.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_()\n",
    "        x = x + self.q_out.zero_point\n",
    "        x.clamp_(0., 2. ** self.num_bits - 1.).round_()\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-projector",
   "metadata": {},
   "source": [
    "### 5、量化线性层类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "micro-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinear(QModule):\n",
    "    def __init__(self, fc_module, has_qin=True, has_qout=True, num_bits=8):\n",
    "        super(QLinear, self).__init__(has_qin, has_qout, num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.fc_module = fc_module\n",
    "        self.q_weight = QParam(num_bits=num_bits)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        self.q_weight.update(self.fc_module.weight.data)\n",
    "        x = F.linear(input = x, \n",
    "                     weight = fake_quantize(self.fc_module.weight, self.q_weight), \n",
    "                     bias = self.fc_module.bias)\n",
    "        \n",
    "        if hasattr(self, 'q_out'):\n",
    "            self.q_out.update(x)\n",
    "            x = fake_quantize(x, self.q_out)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def freeze(self, q_in=None, q_out=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'q_out') and q_out is not None:\n",
    "            raise ValueError('q_out has been provided in init function.')\n",
    "        if not hasattr(self, 'q_out') and q_out is None:\n",
    "            raise ValueError('q_out is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "        if q_out is not None:\n",
    "            self.q_out = q_out\n",
    "        \n",
    "        # 计算 M = s_w * s_in / s_out\n",
    "        self.M = self.q_weight.scale * self.q_in.scale / self.q_out.scale\n",
    "        \n",
    "        # 量化卷积层中的权重\n",
    "        self.fc_module.weight.data = self.q_weight.quantize_tensor(self.fc_module.weight.data) \\\n",
    "                                        - self.q_weight.zero_point\n",
    "        # 量化卷积层中的偏置\n",
    "        self.fc_module.bias.data = quantize_tensor(self.fc_module.bias.data, scale=self.q_in.scale * self.q_weight.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "        \n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.q_in.zero_point\n",
    "        x = self.fc_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_()\n",
    "        x = x + self.q_out.zero_point\n",
    "        x.clamp_(0., 2. ** self.num_bits - 1.).round_()\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-description",
   "metadata": {},
   "source": [
    "### 6、量化ReLu 类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "placed-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QReLU(QModule):\n",
    "    def __init__(self, has_qin=False, num_bits=None):\n",
    "        super(QReLU, self).__init__(has_qin=has_qin,num_bits=num_bits)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def freeze(self, q_in=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x.clone()\n",
    "        x[x < self.q_in.zero_point] = self.q_in.zero_point\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-reply",
   "metadata": {},
   "source": [
    "### 7、量化最大池化层类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "awful-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMaxPooling2d(QModule):\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=0, has_qin=False, num_bits=None):\n",
    "        super(QMaxPooling2d, self).__init__(has_qin=has_qin, num_bits=num_bits)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        x = F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def freeze(self, q_in=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        return F.max_pool2d(x, self.kernel_size, self.stride, self.padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-uruguay",
   "metadata": {},
   "source": [
    "### 8、卷积BNReLU合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acute-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QConvBNReLU(QModule):\n",
    "#     def __init__(self, conv_module, bn_module, has_qin=True, has_qout=True, num_bits=8):\n",
    "#         super(QConvBNReLU, self).__init__(has_qin, has_qout, num_bits)\n",
    "#         self.num_bits = num_bits\n",
    "#         self.conv_module = conv_module\n",
    "#         self.bn_module = bn_module\n",
    "#         self.q_weight = QParam(num_bits=num_bits)\n",
    "#         self.q_bias = QParam(num_bits=32)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         if hasattr(self, 'q_in'):\n",
    "#             self.q_in.update(x)\n",
    "#             x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "#         if self.training:\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
