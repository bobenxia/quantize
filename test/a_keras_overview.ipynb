{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "discrete-regular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-clock",
   "metadata": {},
   "source": [
    "## 2、构建简单模型\n",
    "### 2.1 模型堆叠\n",
    "首先构建一个序列堆叠的网络模型，使用 `tf.keras.Sequential` 结构初始化 `model`， 并且通过 `model.add` 一层一层堆叠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continued-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(72,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-chinese",
   "metadata": {},
   "source": [
    "### 2.2 layer 参数配置\n",
    "了解一下 `tf.keras.layers`中一系列参数：\n",
    "- `activation`\n",
    "- `kernel_initializer` 和 `bias_initializer` ：\n",
    "    - 创建层权重（核和偏差）的初始化方法。此参数是一个名称或可调用对象，默认为 \"Glorot uniform\" 初始化器。\n",
    "- `kernel_regularizer` 和 `bias_regularizer`：\n",
    "    - 应用层权重（核和偏差）的正则化方案，例如 L1 或 L2 正则化。默认情况下，系统不会应用正则化函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-laundry",
   "metadata": {},
   "source": [
    "## 3、训练和评估\n",
    "### 3.1 设置训练流程\n",
    "现在有了堆叠好的模型，在正式开始训练模型之前，还需要使用 `model.compile` 配置一下训练流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "minus-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "             loss = keras.losses.categorical_crossentropy,\n",
    "             metrics=[keras.metrics.categorical_crossentropy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-database",
   "metadata": {},
   "source": [
    "### 3.2 compile 参数配置\n",
    "参数：\n",
    "- `optimizer`:\n",
    "    - 可以在调用 `model.compile()` 之前初始化一个优化器对象，然后传入该函数。也可以在调用 `model.compile()` 时传递一个预定义优化器名。所有优化器可以在括号中输入 `lr` 参数，也可以输入 `clipnorm` 和 `clipvalue` 进行梯度裁剪。预定义的优化器有如下几种：\n",
    "    \n",
    "        - SGD：keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "        - RMSprop：keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
    "        - Adagrad：keras.optimizers.Adagrad(lr=0.01, epsilon=1e-06)\n",
    "        - Adadelta：keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-06)\n",
    "        - Adam：keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        - Adamax：keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        - Nadam：keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "        \n",
    "- `loss`\n",
    "- `metrics`:\n",
    "    - 与loss类似，不过此时的函数只用于模型性能评估但是不会用于训练，多和loss使用一样的函数。\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-integration",
   "metadata": {},
   "source": [
    "### 3.3 随机生成数据进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-alarm",
   "metadata": {},
   "source": [
    "#### 3.3.1 使用 numpy 输入数据并训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daily-bargain",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 1s 3ms/step - loss: 13.0822 - categorical_crossentropy: 13.0822 - val_loss: 23.7198 - val_categorical_crossentropy: 23.7198\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 932us/step - loss: 28.4171 - categorical_crossentropy: 28.4171 - val_loss: 48.4061 - val_categorical_crossentropy: 48.4061\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 903us/step - loss: 53.5981 - categorical_crossentropy: 53.5981 - val_loss: 61.7865 - val_categorical_crossentropy: 61.7865\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 908us/step - loss: 45.9601 - categorical_crossentropy: 45.9601 - val_loss: 24.5644 - val_categorical_crossentropy: 24.5644\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 900us/step - loss: 42.3531 - categorical_crossentropy: 42.3531 - val_loss: 104.8015 - val_categorical_crossentropy: 104.8015\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 895us/step - loss: 92.9879 - categorical_crossentropy: 92.9879 - val_loss: 70.9360 - val_categorical_crossentropy: 70.9360\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 878us/step - loss: 152.3000 - categorical_crossentropy: 152.3000 - val_loss: 198.4955 - val_categorical_crossentropy: 198.4955\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 896us/step - loss: 206.3109 - categorical_crossentropy: 206.3109 - val_loss: 330.4597 - val_categorical_crossentropy: 330.4597\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 918us/step - loss: 277.7376 - categorical_crossentropy: 277.7376 - val_loss: 323.0447 - val_categorical_crossentropy: 323.0447\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 894us/step - loss: 374.0918 - categorical_crossentropy: 374.0918 - val_loss: 256.9105 - val_categorical_crossentropy: 256.9105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4e200eff50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = np.random.random((1000, 72))\n",
    "train_y = np.random.random((1000, 10))\n",
    "\n",
    "val_x = np.random.random((200, 72))\n",
    "val_y = np.random.random((200, 10))\n",
    "\n",
    "model.fit(train_x, train_y, epochs=10,batch_size=10, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-bishop",
   "metadata": {},
   "source": [
    "#### 3.3.2 使用 tf.data 输入数据并训练\n",
    "除了使用 numpy 输入，也可以通过 `tf.data`将训练和验证集拼接够输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hydraulic-flush",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 489.3187 - categorical_crossentropy: 489.3187 - val_loss: 781.5093 - val_categorical_crossentropy: 781.5093\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 419.5004 - categorical_crossentropy: 419.5004 - val_loss: 379.0138 - val_categorical_crossentropy: 379.0138\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 408.3172 - categorical_crossentropy: 408.3172 - val_loss: 596.7084 - val_categorical_crossentropy: 596.7084\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 479.1322 - categorical_crossentropy: 479.1322 - val_loss: 422.1902 - val_categorical_crossentropy: 422.1902\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 402.6422 - categorical_crossentropy: 402.6422 - val_loss: 893.9067 - val_categorical_crossentropy: 893.9067\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 673.0088 - categorical_crossentropy: 673.0088 - val_loss: 404.8095 - val_categorical_crossentropy: 404.8095\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 422.3967 - categorical_crossentropy: 422.3967 - val_loss: 507.5520 - val_categorical_crossentropy: 507.5520\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 517.0851 - categorical_crossentropy: 517.0851 - val_loss: 492.5704 - val_categorical_crossentropy: 492.5704\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 498.5277 - categorical_crossentropy: 498.5277 - val_loss: 485.6197 - val_categorical_crossentropy: 485.6197\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 574.4269 - categorical_crossentropy: 574.4269 - val_loss: 824.5052 - val_categorical_crossentropy: 824.5052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4dcb776950>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "dataset = dataset.batch(32).repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y))\n",
    "val_dataset = val_dataset.batch(32).repeat()\n",
    "\n",
    "model.fit(dataset, epochs=10, steps_per_epoch=30, \n",
    "          validation_data=val_dataset, validation_steps=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-hierarchy",
   "metadata": {},
   "source": [
    "#### 3.3.3  model.fit 参数配置\n",
    "- x：输入数据。numpy array\n",
    "- y：标签，numpy array\n",
    "- batch_size\n",
    "- epochs\n",
    "- steps_per_epoch：整数，含义为每个 epoch 分割成多少个 batch_size\n",
    "- validation_data：形式为 （x, y）的 tuple，是指定的验证集\n",
    "- shuffle：是否打乱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-boundary",
   "metadata": {},
   "source": [
    "### 3.4 评估与预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adequate-ozone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 807.1142 - categorical_crossentropy: 807.1142\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 805.8557 - categorical_crossentropy: 805.8557\n"
     ]
    }
   ],
   "source": [
    "test_x = np.random.random((1000, 72))\n",
    "test_y = np.random.random((1000, 10))\n",
    "\n",
    "model.evaluate(test_x, test_y, batch_size=32)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "test_data = test_data.batch(32).repeat()\n",
    "\n",
    "model.evaluate(test_data, steps=30)\n",
    "\n",
    "result = model.predict(test_x, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-default",
   "metadata": {},
   "source": [
    "## 4、保存和恢复"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-writing",
   "metadata": {},
   "source": [
    "### 4.1 权重的保存和读取\n",
    "通过 `model.save_weights`和`model.load_weights`来进行权重的保存和读取，参数为文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "given-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./weights/only_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "twenty-christmas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4dcb68ad10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./weights/only_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "egyptian-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./weights/only_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "weird-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./weights/only_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-better",
   "metadata": {},
   "source": [
    "### 4.2 保存整个模型\n",
    "可以通过 `model.save` 和 `tf.keras.model.load_model` 命令保存和读取整个模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abstract-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/all_models.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "flush-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./models/all_models.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
