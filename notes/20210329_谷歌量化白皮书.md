# 0、谷歌量化白皮书 Quantizing deep convolutional networks forefficient inference: A whitepaper

> - 原文：https://arxiv.org/pdf/1806.08342.pdf
> - 翻译：https://blog.csdn.net/guvcolie/article/details/81286349
> - https://blog.csdn.net/gcf_uinque/article/details/104464660

这篇是介绍量化的综述，对量化的动机、方法等进行介绍

- 1、量化器的设计。作者介绍了三种量化方案：

# 1、常见的量化设计方案

## 1.1 Uniform Affine Quantizer 均匀仿射量化

假设有一个浮点型变量，它的取值范围为（Xmin, Xmax）,现在我们要把它量化到（0， N levels - 1）取值范围，其中对于8bit精度来讲N levels = 256。

我们需要两个参数：量化尺度 scale 和零点 zero point。尺度 scale 决定量化步长，浮点数 0 映射到零点。

- 对于单边分布，范围需要进一步放宽取包含0点。例如，范围为（2.1，3.5）的浮点型变量将会放宽为（0，3.5），然后再量化。注意，这种方式对于极端的单边分布会产生精度损失，例如（150，170）

一旦尺度和零点定义好了，量化过程如下：

<img src="https://img-blog.csdn.net/20180730180344980?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:47%;" />

逆量化过程如下：

<img src="https://img-blog.csdn.net/20180730180540677?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

## 1.2 Uniform symmetric quantizer 均匀对称量化

映射量化的一个简化版本是对称量化，这种方法的 zero point 即 0 。所以公式改为：

<img src="https://img-blog.csdn.net/20180731104843525?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

对于更快的 SIMD（Single Instruction Multiple Data 单指令流多数据流）实现，需要进一步约束权重的范围。这时：

<img src="https://img-blog.csdn.net/20180731105345219?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

逆量化过程如下：

<img src="https://img-blog.csdn.net/20180731105537844?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

## 1.3 Stochastic quantizer

随机量化方法将量化建模为一种额外的噪声，再四舍五入。随机量化可以表示为：

<img src="https://img-blog.csdn.net/2018073111010431?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

由于大多数用于推理的硬件平台不支持这种量化方法，所以我们在推理预测时不使用随机量化方法。

# 2、量化推理：表现和精度

## 2.1 Post Training Quantization（训练后量化）

### 2.1.1 Weight only quantization（只对权重量化）

这种量化只考虑权重量化，目的主要是为了方便传输和存储而减小模型大小，而不考虑在预测时浮点型计算的性能开销。这种量化方法还是很有用的。

### 2.1.2 Quantizing weights and activations（量化权重和激活输出）

我们可以通过计算所有将要被量化的数据的量化参数，来将一个浮点型模型量化为一个8bit精度的整型模型。由于激活输出需要量化，这时我们就得需要标定数据了，并且需要计算激活输出的动态范围。一般使用100个小批量数据就足够估算出激活输出的动态范围了。

### 2.1.3 实验

**1、只量化权重**

<img src="https://img-blog.csdn.net/20180731141142608?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

实验指出：非对称的，逐通道的表现更好

**2、量化权重和激活函数**

<img src="https://img-blog.csdn.net/20180731142312710?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

我们发现以下规律：

- **逐通道量化方法可以提供很好的精度，可以作为权重和激活输出的先训练后量化的很好的基线，而且非对称量化方式对于所有模型都能提高和浮点型很相近的精度。**
- **激活输出做8bit量化后，精度几乎没有损失。由于以下原因，激活输出的动态范围一般很小：**
  - **无缩放的批归一化：它可以确保所有特征的激活输出服从均值为0，方差为1的分布。**
  - **ReLU6: 它可以将所有的特征输出都固定到（0， 6）范围内，从而消除大动态范围的可能。**
- **像Resnets 和 Inception-v3 这种有更多参数的模型与Mobilenets 这种参数较少的模型相比，对于量化的鲁棒性会更高。**
- **当采用逐层量化的方式对权重进行量化时，会有非常大的精度损失，尤其对于Mobilenet架构。**
- **几乎所有的量化精度损失都是源于权重量化而导致的。**

采用逐层量化的方式对权重进行量化时，会产生很大的精度损失，这是因为bn操作在特征层的各个通道间的动态范围差异非常大。Appendix A对bn产生的影响做了更详尽的讨论。逐层量化可以通过通道粒度来避免这个问题，它可以逐通道地量化而不依赖与bn的缩放。但是，激活输出仍然需要采用逐层对称量化方式来量化。

> 为什么？？？

## 2.2 Quantization Aware Training （训练时量化）

训练时量化方法相比于训练后量化，能够得到更高的精度。

推荐TensorFlow的自动量化工具。

> 不熟悉Tf，没看



### 2.2.1 Operation Transformations for Quantization

确保在训练时所有与量化相关的人为操作都如实地进行建模是非常重要的。这可以使简单的操作，比如add（图5）和concat（图6）操作，变得有意义，因为需要重新调整定点值以使这些操作能够正确地工作。

<img src="https://img-blog.csdn.net/20180801105030536?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

<img src="https://img-blog.csdn.net/20180801105050667?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

除此之外，在训练期间确保在预测时操作符的融合能够正确地建模是很重要的。例如，考虑一个add操作，然后紧跟一个ReLU操作。在这种情况下，大多数硬件平台都支持在预测阶段将add和ReLU进行融合。为了匹配这种情况，假的量化操作运算不应该置于add和ReLU之间。

### 2.2.2 Batch Normalization

在训练阶段，Batch normalization定义如下：

<img src="https://img-blog.csdn.net/20180801105251106?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

在预测阶段，Batch normalization定义如下：

<img src="https://img-blog.csdn.net/20180801105510414?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

其中 μ B 和 σ B分别为batch的均值和标准差。μ 和 σ 是整个训练集的均值和标准差，在训练阶段，这两个值可以通过batch的滑动平均方式计算得到（感兴趣的朋友可以自行了解momentum知识）。

对于预测，我们将bn按照公式21和22所定义的那样折叠进权重中。因此，在预测时没有显式的bn操作。bn按如下方式拆分整合为权值和偏置：

<img src="https://img-blog.csdn.net/20180801110817256?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

<img src="https://img-blog.csdnimg.cn/20190315134417913.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll,size_16,color_FFFFFF,t_70" alt="img" style="zoom:50%;" />

> 这里提到了 推理中 bn 是来自不同批数据整个的均值和标准差，由于批统计量在每个 batch 之间变化很大，这会在量化的权重中引入不希望的跳变。
>
> 给了解决方案，还没看

### 2.2.3 实验

训练时量化精度和浮点型模型精度相差无几，甚至对于权重的逐层量化方式也是如此。我们在训练中，重复做了之前的对权重和激活输出的量化实验，我们从浮点型checkpoint继续训练，并将bn冻结，得到的结果如图11、12和表4所示。

所有实验都采用以下设置：

- 从浮点型checkpoint继续训练，我们使用[26]（slim官方预训练模型）中提高的模型。
- 使用随机梯度下降法训练，学习率1e-5.

我们发现：

- 采用对称量化和非对称量化的训练结果相差很小。
- 即使采用很简单的量化方案，也能达到几乎和浮点型精度一样的精度。甚至逐层量化也是如此。

# 3、Training best practice（训练最佳实践）

我们做了一些不同配置的训练量化模型的实验：首先，我们比较了随机量化（2.3节）和确定性量化（2.3节之外的量化方式）。然后，我们研究了从头开始训练一个量化模型所得到的模型精度是否会比从浮点型checkpoint基础上训练得到的模型精度要高。我们也对量化bn的不同方法进行的评测，结果显示带有修正的bn会产生最好的精度。我们也比较了在训练中对权重进行平均和不进行平均这两种方案。

- 随机量化不会提升精度：随机量化方案会确定浮点型权重，这种浮点型权重在随机量化中提供鲁邦的性能，但这样会导致量化的权重在小批量之间波动变化。**在预测时，量化值是确定的，这就会导致和训练时的不一致。**我们发现正是因为这种不一致，会导致随机量化方案和确定量化方案相比表现不佳（图12），因为确定量化方案在训练过程中能够更好地补偿这种不一致。

- 在浮点型checkpoint基础上量化模型会得到更好的精度：这个问题实际上就是从头开始训练一个量化模型所得到的模型精度是否会比从浮点型checkpoint基础上训练得到的模型精度要高。和其他的研究成果[27]一致，我们发现从浮点型checkpoint基础上继续训练模型会得到更好的精度，如图13。这也和一般的发现规律——最好先以尽可能多的自由度去训练一个模型，然后用这个模型作为引导老师去训练更小的模型——相一致。

- 将bn结构按预测进行匹配会降低数据抖动，提高模型精度。我们展示两个网络的结果。在第一个实验中（图14），我们利用Mobilenet-V1_1_224比较了原始实现的bn和采用纠正和冻结方式的bn。我们发现采用我们提出的方法能得到更稳定、更高的精度。在第二个实验中，我们利用Mobilenet-V2_1_224比较了原始实现的bn和采用纠正和冻结方式的bn。我们发现纠正操作可以稳定训练，冻结可以是精度进一步提升，参见图15中400000步后。

  <img src="https://img-blog.csdn.net/20180801164739992?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

- 慎用指数滑动平均。在浮点型模型训练中，为了提高精度[30]，通常会对权重进行滑动平均操作[29]。由于我们在反向传播时使用的是量化后的权重和激活输出，浮点型权重可能会收敛于量化决策边界的位置上。甚至，瞬时的和滑动平均的浮点型权重之间的微小差异，也会导致量化的权重产生非常明显的不同，这会影响性能，参见图15中EMA（Exponential moving averaging）指数滑动平均对应曲线的精度下降。

  <img src="https://img-blog.csdn.net/20180801164808808?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1dmNvbGll/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img" style="zoom:50%;" />

# 4、Model Architecture Recommendation（模型架构建议）

在本节中，我们探讨激活函数的选择，以及模型精度和模型宽度（其实就是模型尺寸）之间的取舍问题。

- 不要约束激活范围：将ReLU6替换为ReLU，会有微小的精度提升，所以就让模型在训练中自己确定激活范围吧

- 探索模型宽度和量化之间的取舍：冗余度较高的模型更适合量化。即使对于想mobilenet这种更加精简的模型架构，也可以在精度和模型尺寸之间进行取舍。我们拿4bit权重逐通道量化模型 和 8bit不同深度乘子的量化模型（也就是8bit不同容量的量化模型），针对精度进行了比较，如图17。这种比较允许我们评估一种容量和量化之间的权衡关系（参见[31]）。我们发现一个有趣的现象，将权重进行4bit量化后的模型精度 和 直接将模型容量缩减25%的模型精度相差无几。

  