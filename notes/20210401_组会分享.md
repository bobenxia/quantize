# 20210401_组会分享

## 1、量化基本概念

> 回想一下，我们通常会将一张 uint8 类型、数值范围在 0\~255 的图片归一成 float32 类型、数值范围在 0.0\~1.0 的张量，这个过程就是**反量化**。类似地，我们经常将网络输出的范围在 0.0\~1.0 之间的张量调整成数值为 0~255、uint8 类型的图片数据，这个过程就是**量化**。

量化本质上只是对数值范围的重新调整。

映射有线性映射和非线性映射，常用的是线性映射，这次分享也是线性映射。

## 2、基本公式（线性）

用 ![[公式]](https://www.zhihu.com/equation?tex=r) 表示浮点实数，![[公式]](https://www.zhihu.com/equation?tex=q) 表示量化后的定点整数。浮点和整型之间的换算公式为：

![[公式]](https://www.zhihu.com/equation?tex=+r+%3D+S%28q-Z%29+%5Ctag%7B1%7D)

> 可以这样理解：r - 浮点数0 = S(q - 量化后0即Z)

![[公式]](https://www.zhihu.com/equation?tex=q+%3D+round%28%5Cfrac%7Br%7D%7BS%7D%2BZ%29+%5Ctag%7B2%7D)

其中，![[公式]](https://www.zhihu.com/equation?tex=S) 是 scale，表示实数和整数之间的比例关系，![[公式]](https://www.zhihu.com/equation?tex=Z) 是 zero point，表示实数中的 0 经过量化后对应的整数。

它们的计算方法为：

![[公式]](https://www.zhihu.com/equation?tex=S+%3D+%5Cfrac%7Br_%7Bmax%7D-r_%7Bmin%7D%7D%7Bq_%7Bmax%7D-q_%7Bmin%7D%7D+%5Ctag%7B3%7D)

![[公式]](https://www.zhihu.com/equation?tex=Z+%3D+round%28q_%7Bmax%7D+-+%5Cfrac%7Br_%7Bmax%7D%7D%7BS%7D%29+%5Ctag%7B4%7D)

![[公式]](https://www.zhihu.com/equation?tex=r_%7Bmax%7D) 、 ![[公式]](https://www.zhihu.com/equation?tex=r_%7Bmin%7D) 分别是![[公式]](https://www.zhihu.com/equation?tex=r)的最大值和最小值， ![[公式]](https://www.zhihu.com/equation?tex=q_%7Bmin%7D) 、 ![[公式]](https://www.zhihu.com/equation?tex=q_%7Bmax%7D) 同理。



好的，现在我们用代码来实现这个流程：

```python
def calcu_scale_and_zeropoint(min_val, max_val, num_bits=8):
    # 计算 scale 和 zero_point 的基本公式，num_bits表示量化的位数
    # 假设这里是量化为 8bits，并且设定最小值为0， 那么最大值为 2**8-1 = 255
    q_min = 0.
    q_max = 2. ** num_bits - 1
    
    # 计算 scale 和 zero_point
    scale = float((max_val - min_val) / (q_max - q_min))
    zero_point =  np.clip(int(q_max - max_val / scale), q_min, q_max)
    
    return scale, zero_point

```

```python
def quantize_tensor(x, scale, zero_point, num_bits=8 ):
    # 计算 scale 和 zero_point 的基本公式，num_bits表示量化的位数
    # 假设这里是量化为 8bits，并且设定最小值为0， 那么最大值为 2**8-1 = 255
    q_min = 0.
    q_max = 2. ** num_bits -1.
    
    q_x = x / scale + zero_point
    q_x.clamp_(q_min, q_max).round()  # q=round(r/S+Z)
        
    return q_x
```



## 3、卷积中的量化

假设一个这样的网络：

> 只看到 a1，后面忽略

<img src="https://pic1.zhimg.com/80/v2-8aeb50d76358ee1f6e88c33916b57200_720w.jpg" alt="img" style="zoom:50%;" />

假设输入为 ![[公式]](https://www.zhihu.com/equation?tex=x)，我们可以事先统计样本的最大值和最小值，然后计算出![[公式]](https://www.zhihu.com/equation?tex=S_x)(scale) 和![[公式]](https://www.zhihu.com/equation?tex=Z_x) (zero point)。

中间层的 feature map 为![[公式]](https://www.zhihu.com/equation?tex=a_1)，事先统计出它们的 scale 和 zero point 为 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Ba1%7D) 、![[公式]](https://www.zhihu.com/equation?tex=Z_%7Ba1%7D) 。

卷积公式如下：

![[公式]](https://www.zhihu.com/equation?tex=a_1%5E%7Bi%2Ck%7D%3D%5Csum_%7Bj%3D1%7D%5EN+x%5E%7Bi%2Cj%7Dw_1%5E%7Bj%2Ck%7D+%5Ctag%7B9%7D)

现在我们把上面的公式代入，即：

![[公式]](https://www.zhihu.com/equation?tex=+r+%3D+S%28q-Z%29+%5Ctag%7B1%7D)

可以得到：

![[公式]](https://www.zhihu.com/equation?tex=q_%7Ba1%7D%5E%7Bi%2Ck%7D%3DM%5Csum_%7Bj%3D1%7D%5EN%28q_x%5E%7Bi%2Cj%7D-Z_x%29%28q_%7Bw1%7D%5E%7Bj%2Ck%7D-Z_%7Bw1%7D%29%2BZ_%7Ba1%7D+%5Ctag%7B10%7D)

其中 ![[公式]](https://www.zhihu.com/equation?tex=M%3D%5Cfrac%7BS_%7Bw1%7DS_%7Bx%7D%7D%7BS_%7Ba1%7D%7D) 。

> 此时，如果此时是int8的量化，那么int8精度的（qx-zx）替换原先 float32精度的 x，同时int8精度的（qw-zw）替换原先 float32精度的 w，参与卷积运算，然后再乘以M加上输出的 zero_point，就能得到量化后的输出。将量化后的输出反量化，就能得到结果。

看看代码

```python
# 这里是一个量化卷积的定义，省略一些不影响理解流程的代码
class QConv2d(QModule):
    # 这里是一些初始化
    def __init__(self, conv_module, has_qin=True, has_qout=True, num_bits=8):
        super(QConv2d, self).__init__(has_qin, has_qout, num_bits)
        self.num_bits = num_bits
        self.conv_module = conv_module
        self.q_weight = QParam(num_bits=num_bits)
    
    # 这里的目的是为了计算相关的参数，和上面展示计算 max\scale等具有相同的功能
    def forward(self, x):
        if hasattr(self, 'q_in'):
            self.q_in.update(x)
            
        self.q_weight.update(self.conv_module.weight.data)
        x = self.conv_module(x)
        
        if hasattr(self, 'q_out'):
            self.q_out.update(x)
            
        return x
    
    # 这里是进行权值和bias等量化的操作，并将量化后的参数写入到卷积中
    def freeze(self, q_in=None, q_out=None):
        ... ...
        # 计算 M = s_w * s_in / s_out
        self.M = self.q_weight.scale * self.q_in.scale / self.q_out.scale
        
        # 量化卷积层中的权重
        self.conv_module.weight.data = self.q_weight.quantize_tensor(self.conv_module.weight.data) \
                                        - self.q_weight.zero_point
        # 量化卷积层中的偏置
        self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data, scale=self.q_in.scale * self.q_weight.scale,
                                                     zero_point=0, num_bits=32, signed=True)
        
    # 这里是使用量化的参数进行量化
    def quantize_inference(self, x):
        x = x - self.q_in.zero_point #这里是量化输入
        x = self.conv_module(x) # 使用量化后的输入进行卷积
        x = self.M * x #将卷积结果乘以 M
        x.round_()
        x = x + self.q_out.zero_point # 将上一步的结果加上输出的zero_point
        x.clamp_(0., 2. ** self.num_bits - 1.).round_() # 限制范围
        return x # 注意这里的输出结果并不是最后的结果，是量化版的最后结果。需要反量化才能得到最后的结果
```



## 4、逐层量化和逐通道量化

根据量化的粒度（共享量化参数的范围）可以分为逐层、逐组和逐通道--

- 逐层量化，整个layer的权重共用一组scale和 zero_point
- 逐组量化，每个group使用一组 s 和 z


