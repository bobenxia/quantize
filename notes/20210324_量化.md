# 量化

# 1、量化入门-基本原理

> https://zhuanlan.zhihu.com/p/149659607
>
> https://github.com/google/gemmlowp/blob/master/doc/low-precision.md#efficient-handling-of-offsets

目前学术界的量化方法都过于花俏，能落地的极少，工业界广泛使用的还是 Google TFLite 那一套量化方法，而 TFLite 对应的大部分资料都只告诉你如何使用，能讲清楚原理的也非常少。这系列教程不会涉及学术上那些花俏的量化方法，主要是想介绍工业界用得最多的量化方案 (即 TFLite 的量化原理，对应 Google 的论文 [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1712.05877) )

## 1.1 背景知识

我们在对图像做预处理时就用到了量化。

> 回想一下，我们通常会将一张 uint8 类型、数值范围在 0~255 的图片归一成 float32 类型、数值范围在 0.0~1.0 的张量，这个过程就是**反量化**。类似地，我们经常将网络输出的范围在 0.0~1.0 之间的张量调整成数值为 0~255、uint8 类型的图片数据，这个过程就是**量化**。

所以量化本质上只是对数值范围的重新调整。

可以「粗略」理解为是一种线性映射。(之所以加「粗略」二字，是因为有些论文会用非线性量化，但目前在工业界落地的还都是线性量化，所以本文只讨论线性量化的方案)。

---

可以明显看出，反量化一般没有信息损失，而量化一般都会有精度损失。这也非常好理解，float32 能保存的数值范围本身就比 uint8 多，因此必定有大量数值无法用 uint8 表示，只能四舍五入成 uint8 型的数值。

量化模型和全精度模型的误差也来自四舍五入的 clip 操作。

---

这篇文章中会用到一些公式，这里我们用 ![[公式]](https://www.zhihu.com/equation?tex=r) 表示浮点实数，![[公式]](https://www.zhihu.com/equation?tex=q) 表示量化后的定点整数。浮点和整型之间的换算公式为：

![[公式]](https://www.zhihu.com/equation?tex=+r+%3D+S%28q-Z%29+%5Ctag%7B1%7D)

![[公式]](https://www.zhihu.com/equation?tex=q+%3D+round%28%5Cfrac%7Br%7D%7BS%7D%2BZ%29+%5Ctag%7B2%7D)

其中，![[公式]](https://www.zhihu.com/equation?tex=S) 是 scale，表示实数和整数之间的比例关系，![[公式]](https://www.zhihu.com/equation?tex=Z) 是 zero point，表示实数中的 0 经过量化后对应的整数，它们的计算方法为：

![[公式]](https://www.zhihu.com/equation?tex=S+%3D+%5Cfrac%7Br_%7Bmax%7D-r_%7Bmin%7D%7D%7Bq_%7Bmax%7D-q_%7Bmin%7D%7D+%5Ctag%7B3%7D)

![[公式]](https://www.zhihu.com/equation?tex=Z+%3D+round%28q_%7Bmax%7D+-+%5Cfrac%7Br_%7Bmax%7D%7D%7BS%7D%29+%5Ctag%7B4%7D)

![[公式]](https://www.zhihu.com/equation?tex=r_%7Bmax%7D) 、 ![[公式]](https://www.zhihu.com/equation?tex=r_%7Bmin%7D) 分别是![[公式]](https://www.zhihu.com/equation?tex=r)的最大值和最小值， ![[公式]](https://www.zhihu.com/equation?tex=q_%7Bmin%7D) 、 ![[公式]](https://www.zhihu.com/equation?tex=q_%7Bmax%7D) 同理。这个公式的推导比较简单，很多资料也有详细的介绍，这里不过多介绍。需要强调的一点是，定点整数的 zero point 就代表浮点实数的 0，二者之间的换算不存在精度损失，这一点可以从公式 (2) 中看出来，把 ![[公式]](https://www.zhihu.com/equation?tex=r%3D0) 代入后就可以得到 ![[公式]](https://www.zhihu.com/equation?tex=q%3DZ)。这么做的目的是为了在 padding 时保证浮点数值的 0 和定点整数的 zero point 完全等价，保证定点和浮点之间的表征能够一致。

---

## 1.2 矩阵运算的量化

由于卷积网络中的卷积层和全连接层本质上都是一堆矩阵乘法，因此我们先看如何将浮点运算上的矩阵转换为定点运算。

假设 ![[公式]](https://www.zhihu.com/equation?tex=r_1)、![[公式]](https://www.zhihu.com/equation?tex=r_2) 是浮点实数上的两个![[公式]](https://www.zhihu.com/equation?tex=N+%5Ctimes+N)的矩阵，![[公式]](https://www.zhihu.com/equation?tex=r_3) 是 ![[公式]](https://www.zhihu.com/equation?tex=r_1)、![[公式]](https://www.zhihu.com/equation?tex=r_2) 相乘后的矩阵：

![[公式]](https://www.zhihu.com/equation?tex=r_3%5E%7Bi%2Ck%7D%3D%5Csum_%7Bj%3D1%7D%5EN+r_1%5E%7Bi%2Cj%7Dr_2%5E%7Bj%2Ck%7D+%5Ctag%7B5%7D)

假设 ![[公式]](https://www.zhihu.com/equation?tex=S_1)、![[公式]](https://www.zhihu.com/equation?tex=Z_1) 是![[公式]](https://www.zhihu.com/equation?tex=r_1)矩阵对应的 scale 和 zero point， ![[公式]](https://www.zhihu.com/equation?tex=S_2) 、 ![[公式]](https://www.zhihu.com/equation?tex=Z_2) 、 ![[公式]](https://www.zhihu.com/equation?tex=S_3) 、 ![[公式]](https://www.zhihu.com/equation?tex=Z_3) 同理，那么由 (5) 式可以推出：

![[公式]](https://www.zhihu.com/equation?tex=S_3%28q_3%5E%7Bi%2Ck%7D-Z_3%29%3D%5Csum_%7Bj%3D1%7D%5E%7BN%7DS_1%28q_%7B1%7D%5E%7Bi%2Cj%7D-Z_1%29S_2%28q_2%5E%7Bj%2Ck%7D-Z_2%29++%5Ctag%7B6%7D)

整理一下可以得到：

![[公式]](https://www.zhihu.com/equation?tex=q_3%5E%7Bi%2Ck%7D%3D%5Cfrac%7BS_1+S_2%7D%7BS_3%7D%5Csum_%7Bj%3D1%7D%5EN%28q_1%5E%7Bi%2Cj%7D-Z_1%29%28q_2%5E%7Bj%2Ck%7D-Z_2%29%2BZ_3+%5Ctag%7B7%7D)

仔细观察 (7) 式可以发现，除了 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7BS_1+S_2%7D%7BS_3%7D) ，其他都是定点整数运算。那如何把 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7BS_1+S_2%7D%7BS_3%7D) 也变成定点运算呢？

---

这里要用到一个 trick。假设 ![[公式]](https://www.zhihu.com/equation?tex=M%3D%5Cfrac%7BS_1+S_2%7D%7BS_3%7D)，由于![[公式]](https://www.zhihu.com/equation?tex=M)通常都是 (0, 1) 之间的实数 (这是通过大量实验统计出来的)，因此可以表示成 ![[公式]](https://www.zhihu.com/equation?tex=M%3D2%5E%7B-n%7DM_0)，其中 ![[公式]](https://www.zhihu.com/equation?tex=M_0)是一个定点实数。注意，定点数并不一定是整数，所谓定点，指的是小数点的位置是固定的，即小数位数是固定的。因此，如果存在 ![[公式]](https://www.zhihu.com/equation?tex=M%3D2%5E%7B-n%7DM_0)，那我们就可以通过 ![[公式]](https://www.zhihu.com/equation?tex=M_0) 的 bit 位移操作实现 ![[公式]](https://www.zhihu.com/equation?tex=2%5E%7B-n%7DM_0)，这样整个过程就都在定点上计算了。

很多刚接触量化的同学对这一点比较疑惑，下面我就用一个简单的示例说明这一点。我们把 ![[公式]](https://www.zhihu.com/equation?tex=M%3D%5Cfrac%7BS_1+S_2%7D%7BS_3%7D) 代入 (7) 式可以得到：

![[公式]](https://www.zhihu.com/equation?tex=q_3%5E%7Bi%2Ck%7D%3DM%5Csum_%7Bj%3D1%7D%5EN%28q_1%5E%7Bi%2Cj%7D-Z_1%29%28q_2%5E%7Bj%2Ck%7D-Z_2%29%2BZ_3%3DMP%2BZ_3+%5Ctag%7B8%7D)

这里面![[公式]](https://www.zhihu.com/equation?tex=P)是一个在定点域上计算好的整数。

假设 ![[公式]](https://www.zhihu.com/equation?tex=P%3D7091)，![[公式]](https://www.zhihu.com/equation?tex=M%3D0.0072474273418460) (![[公式]](https://www.zhihu.com/equation?tex=M) 可以通过![[公式]](https://www.zhihu.com/equation?tex=S)事先计算得到)，那下面我们就是要找到一个![[公式]](https://www.zhihu.com/equation?tex=M_0)和 ![[公式]](https://www.zhihu.com/equation?tex=n)，使得![[公式]](https://www.zhihu.com/equation?tex=MP%3D2%5E%7B-n%7DM_0+P)成立。我们可以用一段代码来找到这两个数：

```python
M = 0.0072474273418460
P = 7091

def multiply(n, M, P):
    result = M * P
    Mo = int(round(2 ** n * M)) # 这里不一定要四舍五入截断，因为python定点数不好表示才这样处理

    approx_result = (Mo * P) >> n
    print("n=%d, Mo=%d, approx=%f, error=%f"%\
          (n, Mo, approx_result, result-approx_result))

for n in range(1, 16):
    multiply(n, M, P)
```

输出：

```text
n=1, Mo=0, approx=0.000000, error=51.391507
n=2, Mo=0, approx=0.000000, error=51.391507
n=3, Mo=0, approx=0.000000, error=51.391507
n=4, Mo=0, approx=0.000000, error=51.391507
n=5, Mo=0, approx=0.000000, error=51.391507
n=6, Mo=0, approx=0.000000, error=51.391507
n=7, Mo=1, approx=55.000000, error=-3.608493
n=8, Mo=2, approx=55.000000, error=-3.608493
n=9, Mo=4, approx=55.000000, error=-3.608493
n=10, Mo=7, approx=48.000000, error=3.391507
n=11, Mo=15, approx=51.000000, error=0.391507
n=12, Mo=30, approx=51.000000, error=0.391507
n=13, Mo=59, approx=51.000000, error=0.391507
n=14, Mo=119, approx=51.000000, error=0.391507
n=15, Mo=237, approx=51.000000, error=0.391507
```

可以看到，在 n=11、 ![[公式]](https://www.zhihu.com/equation?tex=M_0%3D15) 的时候，误差就已经在 1 以内了。因此，可以通过对![[公式]](https://www.zhihu.com/equation?tex=M_0P)右移 n 个 bit 来近似![[公式]](https://www.zhihu.com/equation?tex=MP)，而这个误差本身在可以接受的范围内。这样一来，(8) 式就可以完全通过定点运算来计算，即我们实现了浮点矩阵乘法的量化。

---

## 1.3 卷积网络的量化

有了上面矩阵乘法的量化，我们就可以进一步尝试对卷积网络的量化。

假设一个这样的网络：

<img src="https://pic1.zhimg.com/80/v2-8aeb50d76358ee1f6e88c33916b57200_720w.jpg" alt="img" style="zoom:50%;" />



这个网络只有三个模块，现在需要把 conv、fc、relu 量化。

---

假设输入为 ![[公式]](https://www.zhihu.com/equation?tex=x)，我们可以事先统计样本的最大值和最小值，然后计算出![[公式]](https://www.zhihu.com/equation?tex=S_x)(scale) 和![[公式]](https://www.zhihu.com/equation?tex=Z_x) (zero point)。

同样地，假设 conv、fc 的参数为![[公式]](https://www.zhihu.com/equation?tex=w_1)、![[公式]](https://www.zhihu.com/equation?tex=w_2)，以及 scale 和 zero point 为![[公式]](https://www.zhihu.com/equation?tex=S_%7Bw1%7D)、![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bw1%7D) 、![[公式]](https://www.zhihu.com/equation?tex=S_%7Bw2%7D) 、![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bw2%7D) 。中间层的 feature map 为![[公式]](https://www.zhihu.com/equation?tex=a_1)、![[公式]](https://www.zhihu.com/equation?tex=a_2) ，并且事先统计出它们的 scale 和 zero point 为 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Ba1%7D) 、![[公式]](https://www.zhihu.com/equation?tex=Z_%7Ba1%7D) 、![[公式]](https://www.zhihu.com/equation?tex=S_%7Ba2%7D) 、![[公式]](https://www.zhihu.com/equation?tex=Z_%7Ba2%7D) 。

卷积运算和全连接层的本质都是矩阵运算，因此我们可以把卷积运算表示成 (这里先忽略加 bias 的操作，这一步同样可以量化，不过中间有一些 trick，我们在之后的文章再仔细研究)： 

![[公式]](https://www.zhihu.com/equation?tex=a_1%5E%7Bi%2Ck%7D%3D%5Csum_%7Bj%3D1%7D%5EN+x%5E%7Bi%2Cj%7Dw_1%5E%7Bj%2Ck%7D+%5Ctag%7B9%7D)

根据之前的转换，我们可以得到：

![[公式]](https://www.zhihu.com/equation?tex=q_%7Ba1%7D%5E%7Bi%2Ck%7D%3DM%5Csum_%7Bj%3D1%7D%5EN%28q_x%5E%7Bi%2Cj%7D-Z_x%29%28q_%7Bw1%7D%5E%7Bj%2Ck%7D-Z_%7Bw1%7D%29%2BZ_%7Ba1%7D+%5Ctag%7B10%7D)

其中 ![[公式]](https://www.zhihu.com/equation?tex=M%3D%5Cfrac%7BS_%7Bw1%7DS_%7Bx%7D%7D%7BS_%7Ba1%7D%7D) 。

得到 conv 的输出后，我们不用反量化回 ![[公式]](https://www.zhihu.com/equation?tex=a_1)，直接用![[公式]](https://www.zhihu.com/equation?tex=q_%7Ba1%7D)继续后面的计算即可。

---

对于量化的 relu 来说，计算公式不再是![[公式]](https://www.zhihu.com/equation?tex=q_%7Ba2%7D%3Dmax%28q_%7Ba1%7D%2C+0%29)，而是 ![[公式]](https://www.zhihu.com/equation?tex=q_%7Ba2%7D%3Dmax%28q_%7Ba1%7D%2CZ_%7Ba1%7D%29) ，并且![[公式]](https://www.zhihu.com/equation?tex=S_%7Ba1%7D%3DS_%7Ba2%7D)， ![[公式]](https://www.zhihu.com/equation?tex=Z_%7Ba1%7D%3DZ_%7Ba2%7D) (为什么是这样，这一点留作思考题)。另外，在实际部署的时候，relu 或者 bn 通常是会整合到 conv 中一起计算的，这一点在之后的文章再讲解。

---

得到![[公式]](https://www.zhihu.com/equation?tex=q_%7Ba2%7D)后，我们可以继续用 (8) 式来计算 fc 层。假设网络输出为 ![[公式]](https://www.zhihu.com/equation?tex=y)，对应的 scale 和 zero point 为 ![[公式]](https://www.zhihu.com/equation?tex=S_y)、 ![[公式]](https://www.zhihu.com/equation?tex=Z_y) ，则量化后的 fc 层可以用如下公式计算：

![[公式]](https://www.zhihu.com/equation?tex=q_%7By%7D%5E%7Bi%2Ck%7D%3DM%5Csum_%7Bj%3D1%7D%5EN%28q_%7Ba2%7D%5E%7Bi%2Cj%7D-Z_%7Ba2%7D%29%28q_%7Bw2%7D%5E%7Bj%2Ck%7D-Z_%7Bw2%7D%29%2BZ_%7By%7D%5Ctag%7B11%7D)

然后通过公式![[公式]](https://www.zhihu.com/equation?tex=y%3DS_y%28q_y-Z_y%29)把结果反量化回去，就可以得到近似原来全精度模型的输出了。

可以看到，上面整个流程都是用定点运算实现的。

**我们在得到全精度的模型后，可以事先统计出 weight 以及中间各个 feature map 的 min、max，并以此计算出 scale 和 zero point，然后把 weight 量化成 int8/int16 型的整数后，整个网络便完成了量化，然后就可以依据上面的流程做量化推理了。**

## 1.4 逐层、逐组和逐通道

> https://www.yuque.com/yahei/hey-yahei/quantization.mxnet2#adf0d253

根据量化的粒度（共享量化参数的范围）可以分为逐层、逐组和逐通道--

- 逐层量化，整个layer的权重共用一组scale和 zero_point
- 逐组量化，每个group使用一组 s 和 z
- 逐通道量化，每个channel单独使用一组 s 和 z

逐通道量化的粒度较细，通常会得到更小的量化误差。通常我们会对权重做逐通道量化，而堆输入（激活）做逐层量化，这是因为对输入做逐层量化对计算不大有好。

> https://www.yuque.com/yahei/hey-yahei/quantization-post_training#12c5j

## 1.5 计算量化参数 s 和 z 的方法

> https://www.yuque.com/yahei/hey-yahei/quantization.mxnet2#320ddecf

1. **指数平滑平均**

   将校准数据集投喂给模型，收集每个量化的层的输出特征图，计算每个batch 的 S 和 Z，通过指数平滑平均更新 S 和 Z

   > https://zhuanlan.zhihu.com/p/65468307

2. 直方图截断

3. KL散度校准

   TensorRT 的校准方式，通过 KL散度（也就是相对熵，用来描述两个分布之间的差异）来评估量化前后分布的差异，搜索并选取 KL 散度最小的量化参数。

   > https://arleyzhang.github.io/articles/923e2c40/

## 1.6 量化分类（训练）

按照是否需要训练划分，量化通常可以分为从头训练、重训练（retrain）、后训练（post-traing）三种

---

# 2、后训练量化

> https://www.yuque.com/yahei/hey-yahei/quantization-post_training

后训练量化，对预训练的网络选择合适的量化操作和校准操作，以实现量化损失的最小化。

## 2.1 优化目标

通常特征图、权重都可以转换成向量的形式来进行优化，所以可以不是一般性地假设我们的优化目标是向量

## 2.2 目标函数

1. L2距离，用于衡量两向量在空间位置上的差异

2. KL距离，用于衡量两向量在分布上的差异

   为向量 a 和 向量 b 统计直方图（注意 bin 数要一致），然后进行归一化操作，使得所有bin 的数值相加后为 1， 也即得到概率分布图，那么向量 a 和 b 的KL距离为：

   ![img](https://cdn.nlark.com/yuque/__latex/b5e27664db8c32d755cbc5b27e207e4a.svg)

   衡量分布差异的误差函数对数据分布比较敏感，通常需要较大的数据量来保证收集的分布与真实分布足够接近，而 L2距离和余弦距离没有这样的要求。

3. 余弦距离，用于衡量两向量在方向上的差异

   ![img](https://cdn.nlark.com/yuque/__latex/7edf72d5d6d26dcfa298dab66a5c663e.svg)

   > https://zhuanlan.zhihu.com/p/157214981

## 2.3 优化对象

1. 最小化（输入/权重）的量化误差

   量化和反量化公式如下：

   ![img](https://cdn.nlark.com/yuque/__latex/28005dae083ff467350ad8aa0d79e227.svg)

   ![img](https://cdn.nlark.com/yuque/__latex/f0d9e9a39d8b485a3e6514965591fcee.svg)

   那么可以定义一个损失函数![img](https://cdn.nlark.com/yuque/__latex/3a74e1a8a96c210fcf5d040aa37456ce.svg)，找到合适参数使用经过量化反量化后的值和原始值差距最小。

2. 最小化输出误差

   卷积输出为 ![img](https://cdn.nlark.com/yuque/__latex/aa01c9c4df73779c68807143be9b7440.svg)，量化后的卷积输出为![img](https://cdn.nlark.com/yuque/__latex/7f7651892519953b16315bf58a381d4a.svg)

   可以直接定义损失函数![img](https://cdn.nlark.com/yuque/__latex/1ef5e044696622aeb9a87ec45708fba5.svg)。



# 3卷积网络量化流程

了解完整个卷积层的量化，现在我们再来完整过一遍卷积网络的量化流程。

我们继续沿用前文的小网络：

<img src="https://pic1.zhimg.com/80/v2-ba5e66c0fadb7e34ee788d3018ac9f20_720w.jpg" alt="img" style="zoom:50%;" />

因此，在最简单的后训练量化算法中，我们会先按照正常的 forward 流程跑一些数据

在这个过程中，统计输入输出以及中间 feature map 的 min、max。

等统计得差不多了，我们就可以根据 min、max 来计算 scale 和 zero point，然后根据公式 (4) 对一些数据项提前计算。

---

之后，在 inference 的时候，我们会先把输入 ![[公式]](https://www.zhihu.com/equation?tex=x) 量化成定点整数 ![[公式]](https://www.zhihu.com/equation?tex=q_x)，然后按照公式 (4) 计算卷积的输出 ![[公式]](https://www.zhihu.com/equation?tex=q_%7Ba1%7D)，这个结果依然是整型的，

然后继续计算 relu 的输出 ![[公式]](https://www.zhihu.com/equation?tex=q_%7Ba2%7D)。

对于 fc 层来说，它本质上也是矩阵运算，因此也可以用公式 (4) 计算，然后得到 ![[公式]](https://www.zhihu.com/equation?tex=q_y)。

最后，根据 fc 层已经计算出来的 scale 和 zero point，推算回浮点实数 ![[公式]](https://www.zhihu.com/equation?tex=y)。除了输入输出的量化和反量化操作，其他流程完全可以用定点运算来完成。

---



# 3、合并 BN 和 ReLU 合并到 Conv 中

## 3.1 BN

BatchNorm 其实就是在每一层输出的时候做了一遍归一化操作

<img src="https://pic2.zhimg.com/80/v2-9c37cb894cc52a3f1168b89a0c95ec1d_1440w.jpg" alt="img" style="zoom:30%;" />

## 3.2 卷积层和 BN 合并

Folding BatchNorm 不是量化才有的操作，在一般的网络中，为了加速网络推理，我们也可以把 BN 合并到 Conv 中。

合并的过程是这样的，假设有一个已经训练好的 Conv 和 BN：

<img src="https://pic1.zhimg.com/80/v2-9a01a3657d1f6453b8111fd3d1359244_1440w.jpg" alt="img" style="zoom:50%;" />

假设 Conv 的 weight 和 bias 分别是 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b)。那么卷积层的输出为：

![[公式]](https://www.zhihu.com/equation?tex=y%3D%5Csum_%7Bi%7D%5EN+w_i+x_i+%2B+b+%5Ctag%7B1%7D+)

图中 BN 层的均值和标准差可以表示为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_%7By%7D)、![[公式]](https://www.zhihu.com/equation?tex=%5Csigma_%7By%7D)，那么根据论文的表述，BN 层的输出为：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+y_%7Bbn%7D%26%3D%5Cgamma+%5Chat%7By%7D%2B%5Cbeta+%5Cnotag+%5C%5C+%26%3D%5Cgamma+%5Cfrac%7By-%5Cmu_y%7D%7B%5Csqrt%7B%5Csigma_y%5E2%2B%5Cepsilon%7D%7D%2B%5Cbeta+%5Ctag%7B2%7D+%5Cend%7Balign%7D+)

然后我们把 (1) 代入 (2) 中可以得到：

![[公式]](https://www.zhihu.com/equation?tex=y_%7Bbn%7D%3D%5Cfrac%7B%5Cgamma%7D%7B%5Csqrt%7B%5Csigma_y%5E2%2B%5Cepsilon%7D%7D%28%5Csum_%7Bi%7D%5EN+w_i+x_i+%2B+b-%5Cmu_y%29%2B%5Cbeta+%5Ctag%7B3%7D+)

我们用 ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%27) 来表示 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cgamma%7D%7B%5Csqrt%7B%5Csigma_y%5E2%2B%5Cepsilon%7D%7D)，那么 (3) 可以简化为：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+y_%7Bbn%7D%26%3D%5Cgamma%27%28%5Csum_%7Bi%7D%5ENw_ix_i%2Bb-%5Cmu_y%29%2B%5Cbeta+%5Cnotag+%5C%5C+%26%3D%5Csum_%7Bi%7D%5EN+%5Cgamma%27w_ix_i%2B%5Cgamma%27%28b-%5Cmu_y%29%2B%5Cbeta+%5Ctag%7B4%7D+%5Cend%7Balign%7D+)

发现没有，(4) 式形式上跟 (1) 式一模一样，因此它本质上也是一个 Conv 运算，我们只需要用 ![[公式]](https://www.zhihu.com/equation?tex=w_i%27%3D%5Cgamma%27w_i) 和 ![[公式]](https://www.zhihu.com/equation?tex=b%27%3D%5Cgamma%27%28b-%5Cmu_y%29%2B%5Cbeta) 来作为原来卷积的 weight 和 bias，就相当于把 BN 的操作合并到了 Conv 里面。实际 inference 的时候，由于 BN 层的参数已经固定了，因此可以把 BN 层 folding 到 Conv 里面，省去 BN 层的计算开销。

## 3.3 卷积层和 BN合并 （量化）

- 如果量化时不想更新 BN 的参数 (比如后训练量化)，那我们就先把 BN 合并到 Conv 中，直接量化新的 Conv 即可。

- 如果量化时需要更新 BN 的参数 (比如量化感知训练)，那也很好处理。Google 把这个流程的心法写在一张图上了

  <img src="https://pic1.zhimg.com/80/v2-509da6350d0c085cd791dfc504ab12a0_1440w.jpg" alt="img" style="zoom:60%;" />

## 3.4 Conv  和 ReLU 合并

在量化中，Conv + ReLU 这样的结构一般也是合并成一个 Conv 进行运算的，而这一点在全精度模型中则办不到。

ReLU 前后应该使用同一个 scale 和 zeropoint。这是因为 ReLU 本身没有做任何的数学运算，只是一个截断函数，如果使用不同的 scale 和 zeropoint，会导致无法量化回 float 域。

看下图这个例子。假设 ReLU 前的数值范围是 ![[公式]](https://www.zhihu.com/equation?tex=r_%7Bin%7D+%5Cin+%5B-1%2C+1%5D)，那么经过 ReLU 后的数值范围是 ![[公式]](https://www.zhihu.com/equation?tex=r_%7Bout%7D+%5Cin+%5B0%2C1%5D)。假设量化到 uint8 类型，即 [0, 255]，那么 ReLU 前后的 scale 分别为 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Bin%7D%3D%5Cfrac%7B2%7D%7B255%7D)、![[公式]](https://www.zhihu.com/equation?tex=S_%7Bout%7D%3D%5Cfrac%7B1%7D%7B255%7D)，zp 分别为 ![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bin%7D%3D128)、![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bout%7D%3D0)。 再假设 ReLU 前的浮点数是 ![[公式]](https://www.zhihu.com/equation?tex=r_%7Bin%7D%3D0.5)，那么经过 ReLU 后的值依然是 0.5。换算成整型的话，ReLU 前的整数是 ![[公式]](https://www.zhihu.com/equation?tex=q_%7Bin%7D%3D192)，由于 ![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bin%7D%3D128)，因此过完 ReLU 后的数值依然是 192。但是，![[公式]](https://www.zhihu.com/equation?tex=S_%7Bout%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bout%7D) 已经发生了变化，因此反量化后的 ![[公式]](https://www.zhihu.com/equation?tex=r_%7Bout%7D) 不再是 0.5，而这不是我们想要的。所以，如果想要保证量化的 ReLU 和浮点型的 ReLU 之间的一致性，就必须保证 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Bin%7D)、![[公式]](https://www.zhihu.com/equation?tex=S_%7Bout%7D) 以及 ![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bin%7D)、![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bout%7D) 是一致的。

<img src="https://pic4.zhimg.com/80/v2-465e3f1edabb2656b16053aea49e3fff_1440w.jpg" alt="img" style="zoom:40%;" />

但是保证前后的 scale 和 zp 一致，没规定一定得用 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Bin%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bin%7D)，我们一样可以用 ReLU 之后的 scale 和 zp。不过，使用哪一个 scale 和 zp，意义完全不一样。如果使用 ReLU 之后的 scale 和 zp，那我们就可以用量化本身的截断功能来实现 ReLU 的作用。

想要理解这一点，需要回顾一下量化的基本公式：

![[公式]](https://www.zhihu.com/equation?tex=q%3Dround%28%5Cfrac%7Br%7D%7BS%7D%2BZ%29+%5Ctag%7B5%7D+)

注意，这里的 round 除了把 float 型四舍五入转成 int 型外，还需要保证 ![[公式]](https://www.zhihu.com/equation?tex=q) 的数值在特定范围内「例如 0～255」，相当于要做一遍 clip 操作。因此，这个公式更准确的写法应该是「假设量化到 uint8 数值」：

![[公式]](https://www.zhihu.com/equation?tex=q%3Dround%28clip%28%5Cfrac%7Br%7D%7BS%7D%2BZ%2C+0%2C+255%29%29+%5Ctag%7B6%7D+)

记住，ReLU 本身就是在做 clip。所以，我们才能用量化的截断功能来模拟 ReLU 的功能。

再举个例子。

![img](https://pic2.zhimg.com/80/v2-57c408d6cefd22674ff21fb10f7333e9_1440w.jpg)

假设有一个上图所示的 Conv+ReLU 的结构，其中，Conv 后的数值范围是 ![[公式]](https://www.zhihu.com/equation?tex=r_%7Bin%7D+%5Cin+%5B-1%2C1%5D)。在前面的文章中，我们都是用 ReLU 前的数值来统计 minmax 并计算 scale 和 zp，并把该 scale 和 zp 沿用到 ReLU 之后。这部分的计算可以参照图中上半部分。

但现在，我们想在 ReLU 之后统计 minmax，并用 ReLU 后的 scale 和 zp 作为 ReLU 前的 scale 和 zp「即 Conv 后面的 scale 和 zp」，结果会怎样呢？

看图中下半部分，假设 Conv 后的数值是 ![[公式]](https://www.zhihu.com/equation?tex=r_%7Bin%7D%3D-0.5)，此时，由于 Conv 之后的 scale 和 zp 变成了 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B255%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=0)，因此，量化的整型数值为：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+q%26%3Dround%28%5Cfrac%7B-0.5%7D%7B%5Cfrac%7B1%7D%7B255%7D%7D%2B0%29+%5Cnotag++%5C%5C++%26%3Dround%28-128%29+%5Cnotag++%5C%5C+%26%3D0++%5Ctag%7B7%7D+%5Cend%7Balign%7D)

注意，上面的量化过程中，我们执行了截断操作，把 ![[公式]](https://www.zhihu.com/equation?tex=q) 从 -128 截断成 0，而这一步本来应该是在 ReLU 里面计算的！然后，我们如果根据 ![[公式]](https://www.zhihu.com/equation?tex=S_%7Bout%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=Z_%7Bout%7D) 反量化回去，就会得到 ![[公式]](https://www.zhihu.com/equation?tex=r_%7Bout%7D%3D0)，而它正是原先 ReLU 计算后得到的数值。

因此，通过在 Conv 后直接使用 ReLU 后的 scale 和 zp，我们实现了将 ReLU 合并到 Conv 里面的过程。

那对于 ReLU 外的其他激活函数，是否可以同样合并到 Conv 里面呢？这取决于其他函数是否也只是在做 clip 操作，例如 ReLU6 也有同样的性质。但对于其他绝大部分函数来说，由于它们本身包含其他数学运算，因此就不具备类似性质。


