{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "north-encoding",
   "metadata": {},
   "source": [
    "# hash_to_test的作用\n",
    "现在虽然可以通过 Ipynb_import.py文件实现在不同 ipynb文件中互相调用模块，但是在某一 ipynb 调试中跳转到ipynb 中的模块的时候，无法给出在其他 ipynb文件中明确位置。\n",
    "\n",
    "因此，将所有要使用的模块合并到这个文件中，方便调试修改。"
   ]
  },
  {
   "attachments": {
    "30b79489-6c86-4382-a669-c99fa32d5667.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "medieval-genre",
   "metadata": {},
   "source": [
    "## 1、后量化训练（线性量化）\n",
    "https://zhuanlan.zhihu.com/p/156835141\n",
    "\n",
    "非对称量化：\n",
    "\n",
    "![image.png](attachment:30b79489-6c86-4382-a669-c99fa32d5667.png)\n",
    "\n",
    "scale:\n",
    "> s = rmax - rmin / qmax - qmin\n",
    "\n",
    "zero point:\n",
    "> z = round(qmax - rmax / s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-swimming",
   "metadata": {},
   "source": [
    "### 1. 量化基本公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "remarkable-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wound-browse",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 计算 scale 和 zero_point 的基本公式\n",
    "# def calcu_scale_and_zeropoint(min_val, max_val, num_bits=8):\n",
    "#     q_min = 0.\n",
    "#     q_max = 2. ** num_bits - 1\n",
    "#     scale = float((max_val - min_val) / (q_max - q_min))\n",
    "#     zero_point = np.clip(int(q_max - max_val / scale), q_min, q_max)\n",
    "    \n",
    "#     return scale, torch.tensor(zero_point)#.to(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "advanced-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcu_scale_and_zeropoint(min_val, max_val, num_bits=8, is_per_channel_quantize=False):\n",
    "    # 计算 scale 和 zero_point 的基本公式\n",
    "    # is_per_channel_quantize 决定是按照逐通道量化还是按照逐层量化来计算 scale 和 zero_point \n",
    "    q_min = 0.\n",
    "    q_max = 2. ** num_bits - 1\n",
    "    \n",
    "    # 匿名函数，计算 scale 和 zero_point\n",
    "    calcu_scale =  lambda max_val, min_val : float((max_val - min_val) / (q_max - q_min))\n",
    "    calcu_zero_point = lambda max_val, scale :  np.clip(int(q_max - max_val / scale), q_min, q_max)\n",
    "    \n",
    "    # 是否是逐通道量化\n",
    "    if  is_per_channel_quantize:\n",
    "        scale = []\n",
    "        zero_point = []\n",
    "\n",
    "        for i in range(len(min_val)):\n",
    "            scale.append(calcu_scale(max_val[i], min_val[i]))\n",
    "            zero_point.append( calcu_zero_point(max_val[i], scale[i]))  \n",
    "            \n",
    "    else:\n",
    "        scale = calcu_scale(max_val, min_val)\n",
    "        zero_point = calcu_zero_point(max_val, scale)\n",
    "        \n",
    "    return scale, zero_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "billion-modem",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # tensor 量化和反量化\n",
    "# def quantize_tensor(x, scale, zero_point, num_bits=8, signed=False):\n",
    "#     if signed:\n",
    "#         q_min = - 2. ** (num_bits - 1)\n",
    "#         q_max = 2. ** (num_bits - 1) - 1.\n",
    "#     else:\n",
    "#         q_min = 0.\n",
    "#         q_max = 2. ** num_bits -1.\n",
    "        \n",
    "#     q_x = x / scale + zero_point\n",
    "#     q_x.clamp_(q_min, q_max).round()  # q=round(r/S+Z)\n",
    "    \n",
    "# #     return q_x.float()   # 由于pytorch不支持int类型的运算，因此我们还是用float来表示整数\n",
    "#     return q_x#.to(torch.uint8)\n",
    "\n",
    "# def dequantize_tensor(q_x, scale, zero_point):\n",
    "#     return scale * (q_x -zero_point)  # r=S(q-Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "indian-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor(x, scale, zero_point, num_bits=8, signed=False, is_per_channel_quantize=False ):\n",
    "    # 计算 scale 和 zero_point 的基本公式\n",
    "    # is_per_channel_quantize 决定是按照逐通道量化还是按照逐层量化\n",
    "    if signed:\n",
    "        q_min = - 2. ** (num_bits - 1)\n",
    "        q_max = 2. ** (num_bits - 1) - 1.\n",
    "    else:\n",
    "        q_min = 0.\n",
    "        q_max = 2. ** num_bits -1.\n",
    "    \n",
    "    if is_per_channel_quantize:\n",
    "        q_x = torch.zeros(x.shape)\n",
    "        for i in range(len(scale)):\n",
    "            q_x[:,i,:,:] = x[:,i,:,:] /scale[i] + zero_point[i]\n",
    "            q_x[:,i,:,:].clamp_(0, 255).round()\n",
    "            \n",
    "    else:\n",
    "        q_x = x / scale + zero_point\n",
    "        q_x.clamp_(q_min, q_max).round()  # q=round(r/S+Z)\n",
    "        \n",
    "    return q_x\n",
    "\n",
    "def dequantize_tensor(q_x, scale, zero_point, is_per_channel_quantize=False ):\n",
    "    \n",
    "    if is_per_channel_quantize:\n",
    "        x = torch.zeros(q_x.shape)\n",
    "        for i in range(len(scale)):\n",
    "            x[:,i,:,:] = scale[i] * (q_x[:,i,:,:] - zero_point[i])\n",
    "            \n",
    "    else:\n",
    "        x = scale * (q_x -zero_point)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alive-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcu_max_and_min(x, min_val,  max_val, is_per_channel_quantize=False ):\n",
    "    # 根据最大值和最小值统计 上下限\n",
    "    calcu_max = lambda x, max_val: max(0, x.max()) if max_val is None \\\n",
    "                                                                else max(0, x.max(), max_val)\n",
    "    calcu_min = lambda x, min_val: min(0, x.min()) if min_val is None \\\n",
    "                                                                else min(0, x.min(), min_val)\n",
    "    if is_per_channel_quantize:\n",
    "#         max_val =[calcu_max(x[:,i,:,:], max_val[i]) for i in range(len(x[0]))]    \n",
    "#         min_val =[calcu_min(x[:,i,:,:], min_val[i]) for i in range(len(x[0]))]    这样写也行，不过读起来差一点\n",
    "        new_max_val = []\n",
    "        new_min_val = []\n",
    "        for i in range(len(x[0])):\n",
    "            if max_val == None or  max_val[i] == None:\n",
    "                new_max_val.append(calcu_max(x[:,i,:,:], None))\n",
    "            else:\n",
    "                new_max_val.append(calcu_max(x[:,i,:,:], max_val[i]))\n",
    "            if min_val == None or  min_val[i] == None:\n",
    "                new_min_val.append(calcu_min(x[:,i,:,:], None))\n",
    "            else:\n",
    "                new_min_val.append(calcu_min(x[:,i,:,:], min_val[i]))\n",
    "            \n",
    "    else:\n",
    "        new_max_val = calcu_max(x, max_val)\n",
    "        new_min_val = calcu_min(x, min_val)\n",
    "        \n",
    "    return new_min_val, new_max_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-webcam",
   "metadata": {},
   "source": [
    "### 2. 封装成类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sonic-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QParam:\n",
    "    def __init__(self, num_bits=8):\n",
    "        self.num_bits = num_bits\n",
    "        self.scale = None\n",
    "        self.zero_point = None\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "    \n",
    "    def update(self, tensor):\n",
    "        self.min, self.max = calcu_max_and_min(tensor, self.min, self.max, is_per_channel_quantize=True )\n",
    "        self.scale, self.zero_point = calcu_scale_and_zeropoint(self.min, self.max, self.num_bits, is_per_channel_quantize=True)\n",
    "        \n",
    "    def quantize_tensor(self, tensor):\n",
    "        return quantize_tensor(tensor, self.scale, self.zero_point, num_bits=self.num_bits, is_per_channel_quantize=True)\n",
    "    \n",
    "    def dequantize_tensor(self, q_x):\n",
    "        return dequantize_tensor(q_x, self.scale, self.zero_point, is_per_channel_quantize=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-jackson",
   "metadata": {},
   "source": [
    "### 3、量化网络基类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "computational-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModule(nn.Module):\n",
    "    def __init__(self, has_qin=True, has_qout=True, num_bits=8):\n",
    "        # 指定量化的位数外，还需指定是否提供量化输入 (qin) 及输出参数 (qout)\n",
    "        # 不是每一个网络模块都需要统计输入的 min、max，大部分中间层都是用上一层的 qout 来作为自己的 qin 的，\n",
    "        # 另外有些中间层的激活函数也是直接用上一层的 qin 来作为自己的 qin 和 qout。\n",
    "        super(QModule, self).__init__()\n",
    "        if has_qin:\n",
    "            self.q_in = QParam(num_bits)\n",
    "        if has_qout:\n",
    "            self.q_out = QParam(num_bits)\n",
    "    \n",
    "    def freeze(self):\n",
    "        # 函数会在统计完 min、max 后发挥作用\n",
    "        # 很多项是可以提前计算好的，freeze 就是把这些项提前固定下来\n",
    "        # 同时也将网络的权重由浮点实数转化为定点整数。\n",
    "        pass\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        # 在量化 inference 的时候会使用\n",
    "        raise NotImplementedError('quantize_inference should be implemented.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-newport",
   "metadata": {},
   "source": [
    "### 4、伪量化定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "operating-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向推导中，先量化再反量化，引入噪声\n",
    "class FakeQuantize(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, qparam):\n",
    "        x = qparam.quantize_tensor(x)\n",
    "        x = qparam.dequantize_tensor(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None\n",
    "\n",
    "fake_quantize = FakeQuantize.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-howard",
   "metadata": {},
   "source": [
    "### 4、量化卷积层类的定义\n",
    "<img src=\"https://www.zhihu.com/equation?tex=a%3D%5Csum_%7Bi%7D%5EN+w_i+x_i%2Bb+%5Ctag%7B1%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />\n",
    "由此得到量化的公式\n",
    "<img src=\"https://www.zhihu.com/equation?tex=S_a+%28q_a-Z_a%29%3D%5Csum_%7Bi%7D%5EN+S_w%28q_w-Z_w%29S_x%28q_x-Z_x%29%2BS_b%28q_b-Z_b%29+%5Ctag%7B2%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />\n",
    "<img src=\"https://www.zhihu.com/equation?tex=q_a%3D%5Cfrac%7BS_w+S_x%7D%7BS_a%7D%5Csum_%7Bi%7D%5EN+%28q_w-Z_w%29%28q_x-Z_x%29%2B%5Cfrac%7BS_b%7D%7BS_a%7D%28q_b-Z_b%29%2BZ_a+%5Ctag%7B3%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />\n",
    "\n",
    "\n",
    "> <img src=\"./image/image.png\" style=\"zoom:60%;\" />\n",
    "\n",
    "经过调整：\n",
    "\n",
    "<img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+q_a%26%3D%5Cfrac%7BS_w+S_x%7D%7BS_a%7D%28%5Csum_%7Bi%7D%5EN%28q_w-Z_w%29%28q_x-Z_x%29%2Bq_b%29%2BZ_a+%5Cnotag+%5C%5C+%26%3DM%28%5Csum_%7Bi%7D%5EN+q_wq_x-%5Csum_i%5EN+q_wZ_x-%5Csum_i%5EN+q_xZ_w%2B%5Csum_i%5ENZ_wZ_x%2Bq_b%29%2BZ_a+%5Ctag%7B4%7D+%5Cend%7Balign%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "universal-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv2d(QModule):\n",
    "    def __init__(self, conv_module, has_qin=True, has_qout=True, num_bits=8):\n",
    "        super(QConv2d, self).__init__(has_qin, has_qout, num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.conv_module = conv_module\n",
    "        self.q_weight = QParam(num_bits=num_bits)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            # 更新量化输入 q_in 的参数\n",
    "            self.q_in.update(x)\n",
    "            # 利用量化输入 q_in 伪量化全精度输入 x，引入噪声\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        self.q_weight.update(self.conv_module.weight.data)\n",
    "        x = F.conv2d(input = x, \n",
    "                     weight = fake_quantize(self.conv_module.weight, self.q_weight), \n",
    "                     bias = self.conv_module.bias, \n",
    "                     stride = self.conv_module.stride,\n",
    "                     padding = self.conv_module.padding, \n",
    "                     dilation=self.conv_module.dilation, \n",
    "                     groups = self.conv_module.groups)\n",
    "        \n",
    "        if hasattr(self, 'q_out'):\n",
    "            self.q_out.update(x)\n",
    "            x = fake_quantize(x, self.q_out)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def freeze(self, q_in=None, q_out=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'q_out') and q_out is not None:\n",
    "            raise ValueError('q_out has been provided in init function.')\n",
    "        if not hasattr(self, 'q_out') and q_out is None:\n",
    "            raise ValueError('q_out is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "        if q_out is not None:\n",
    "            self.q_out = q_out\n",
    "        \n",
    "        # 计算 M = s_w * s_in / s_out\n",
    "        self.M = self.q_weight.scale * self.q_in.scale / self.q_out.scale\n",
    "        \n",
    "        # 量化卷积层中的权重\n",
    "        self.conv_module.weight.data = self.q_weight.quantize_tensor(self.conv_module.weight.data) \\\n",
    "                                        - self.q_weight.zero_point\n",
    "        self.conv_module.weight.data =self.conv_module.weight.data#.to(torch.uint8)\n",
    "        # 量化卷积层中的偏置\n",
    "        self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data, scale=self.q_in.scale * self.q_weight.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "        \n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.q_in.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_()\n",
    "        x = x + self.q_out.zero_point\n",
    "        x.clamp_(0., 2. ** self.num_bits - 1.).round_()\n",
    "        return x#.to(torch.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-photograph",
   "metadata": {},
   "source": [
    "### 5、量化线性层类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "superior-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinear(QModule):\n",
    "    def __init__(self, fc_module, has_qin=True, has_qout=True, num_bits=8):\n",
    "        super(QLinear, self).__init__(has_qin, has_qout, num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.fc_module = fc_module\n",
    "        self.q_weight = QParam(num_bits=num_bits)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        self.q_weight.update(self.fc_module.weight.data)\n",
    "        x = F.linear(input = x, \n",
    "                     weight = fake_quantize(self.fc_module.weight, self.q_weight), \n",
    "                     bias = self.fc_module.bias)\n",
    "        \n",
    "        if hasattr(self, 'q_out'):\n",
    "            self.q_out.update(x)\n",
    "            x = fake_quantize(x, self.q_out)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def freeze(self, q_in=None, q_out=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'q_out') and q_out is not None:\n",
    "            raise ValueError('q_out has been provided in init function.')\n",
    "        if not hasattr(self, 'q_out') and q_out is None:\n",
    "            raise ValueError('q_out is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "        if q_out is not None:\n",
    "            self.q_out = q_out\n",
    "        \n",
    "        # 计算 M = s_w * s_in / s_out\n",
    "        self.M = self.q_weight.scale * self.q_in.scale / self.q_out.scale\n",
    "        \n",
    "        # 量化卷积层中的权重\n",
    "        self.fc_module.weight.data = self.q_weight.quantize_tensor(self.fc_module.weight.data) \\\n",
    "                                        - self.q_weight.zero_point\n",
    "        # 量化卷积层中的偏置\n",
    "        self.fc_module.bias.data = quantize_tensor(self.fc_module.bias.data, scale=self.q_in.scale * self.q_weight.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "        \n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.q_in.zero_point\n",
    "        x = self.fc_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_()\n",
    "        x = x + self.q_out.zero_point\n",
    "        x.clamp_(0., 2. ** self.num_bits - 1.).round_()\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-pickup",
   "metadata": {},
   "source": [
    "### 6、量化ReLu 类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "imposed-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QReLU(QModule):\n",
    "    def __init__(self, has_qin=False, num_bits=None):\n",
    "        super(QReLU, self).__init__(has_qin=has_qin,num_bits=num_bits)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def freeze(self, q_in=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x.clone()\n",
    "        x[x < self.q_in.zero_point] = self.q_in.zero_point\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-testament",
   "metadata": {},
   "source": [
    "### 7、量化最大池化层类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "living-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMaxPooling2d(QModule):\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=0, has_qin=False, num_bits=None):\n",
    "        super(QMaxPooling2d, self).__init__(has_qin=has_qin, num_bits=num_bits)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        x = F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def freeze(self, q_in=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        return F.max_pool2d(x, self.kernel_size, self.stride, self.padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-knitting",
   "metadata": {},
   "source": [
    "### 8、卷积BNReLU合并\n",
    "卷积层的输出为：\n",
    "<img src=\"https://www.zhihu.com/equation?tex=y%3D%5Csum_%7Bi%7D%5EN+w_i+x_i+%2B+b+%5Ctag%7B1%7D+\" alt=\"[公式]\" style=\"zoom:100%;\" />\n",
    "\n",
    "BN层的输出为：\n",
    "<img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+y_%7Bbn%7D%26%3D%5Cgamma+%5Chat%7By%7D%2B%5Cbeta+%5Cnotag+%5C%5C+%26%3D%5Cgamma+%5Cfrac%7By-%5Cmu_y%7D%7B%5Csqrt%7B%5Csigma_y%5E2%2B%5Cepsilon%7D%7D%2B%5Cbeta+%5Ctag%7B2%7D+%5Cend%7Balign%7D+\" alt=\"[公式]\" style=\"zoom:100%;\" />\n",
    "\n",
    "代入：\n",
    "\n",
    "<img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+y_%7Bbn%7D%26%3D%5Cfrac%7B%5Cgamma%7D%7B%5Csqrt%7B%5Csigma_y%5E2%2B%5Cepsilon%7D%7D%28%5Csum_%7Bi%7D%5EN+w_i+x_i+%2B+b-%5Cmu_y%29%2B%5Cbeta+%5Cnotag+%5C%5C+%26%3D%5Cgamma%27%28%5Csum_%7Bi%7D%5ENw_ix_i%2Bb-%5Cmu_y%29%2B%5Cbeta+%5Cnotag+%5C%5C+%26%3D%5Csum_%7Bi%7D%5EN+%5Cgamma%27w_ix_i%2B%5Cgamma%27%28b-%5Cmu_y%29%2B%5Cbeta+%5Ctag%7B1%7D+%5Cend%7Balign%7D+\" alt=\"[公式]\" style=\"zoom:100%;\" />\n",
    "\n",
    "我们只需要用 <img src=\"https://www.zhihu.com/equation?tex=w_i%27%3D%5Cgamma%27w_i\" alt=\"[公式]\" style=\"zoom:80%;\" /> 和  <img src=\"https://www.zhihu.com/equation?tex=b%27%3D%5Cgamma%27%28b-%5Cmu_y%29%2B%5Cbeta\" alt=\"[公式]\" style=\"zoom:80%;\" />  来作为原来卷积的 weight 和 bias，就相当于把 BN 的操作合并到了 Conv 里面。实际 inference 的时候，由于 BN 层的参数已经固定了，因此可以把 BN 层 folding 到 Conv 里面，省去 BN 层的计算开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "according-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConvBNReLU(QModule):\n",
    "    def __init__(self, conv_module, bn_module, has_qin=True, has_qout=True, num_bits=8):\n",
    "        super(QConvBNReLU, self).__init__(has_qin, has_qout, num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.conv_module = conv_module\n",
    "        self.bn_module = bn_module\n",
    "        self.q_weight = QParam(num_bits=num_bits)\n",
    "        self.q_bias = QParam(num_bits=32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        if self.training:  # BN层训练，更新\n",
    "            y = F.conv2d(input = x, \n",
    "                     weight = self.conv_module.weight, \n",
    "                     bias = self.conv_module.bias, \n",
    "                     stride = self.conv_module.stride,\n",
    "                     padding = self.conv_module.padding, \n",
    "                     dilation=self.conv_module.dilation, \n",
    "                     groups = self.conv_module.groups)\n",
    "            # 改变维度，为了方便channel wise计算均值和方差\n",
    "            y = y.permute(1, 0, 2, 3)  # NCHW -> CNHW\n",
    "            y = y.contiguous().view(self.conv_module.out_channels, -1)  # CNHW -> C, NHW，\n",
    "            # 计算通道维上的均值和方差\n",
    "            mean = y.mean(1)\n",
    "            var = y.var(1)\n",
    "            # 通过移动平均更新整个数据集样本的均值和方差\n",
    "            self.bn_module.running_mean =  self.bn_module.momentum * self.bn_module.running_mean + \\\n",
    "                                                                                                                    (1 - self.bn_module.momentum) * mean\n",
    "            self.bn_module.running_var = self.bn_module.momentum * self.bn_module.running_var + \\\n",
    "                                                                                                                     (1 - self.bn_module.momentum) * var\n",
    "        else:  # BN层不训练\n",
    "            mean = self.bn_module.running_mean\n",
    "            var = self.bn_module.running_var\n",
    "            \n",
    "        #  标准差 std\n",
    "        std = torch.sqrt(var, self.bn_module.eps)\n",
    "        \n",
    "        # 按照上述公式，进行合并操作，获取新的合并后的权值和偏置\n",
    "        new_weight, new_bias = self.fold_bn(mean, std)\n",
    "        self.q_weight.update(new_weight.data)\n",
    "        \n",
    "        x = F.conv2d(input = x, \n",
    "                     weight = fake_quantize(new_weight, self.q_weight), \n",
    "                     bias = new_bias, \n",
    "                     stride = self.conv_module.stride,\n",
    "                     padding = self.conv_module.padding, \n",
    "                     dilation=self.conv_module.dilation, \n",
    "                     groups = self.conv_module.groups)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        if hasattr(self, 'q_out'):\n",
    "            self.q_out.update(x)\n",
    "            x = fake_quantize(x, self.q_out)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def fold_bn(self, mean, std):\n",
    "        if self.bn_module.affine:\n",
    "            gamma_ = self.bn_module.weight / std\n",
    "            weight = self.conv_module.weight * gamma_.view(self.conv_module.out_channels, 1, 1, 1)\n",
    "            if self.conv_modules.bias is not None:\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean + self.bn_module.bias\n",
    "            else:\n",
    "                bias = self.bn_module.bias - gamma_ * mean\n",
    "        else:\n",
    "            gamma_ = 1 / std\n",
    "            weight = self.conv_module.weight * gamma_\n",
    "            if self.conv_module.bias is not None:\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean\n",
    "            else:\n",
    "                bias = -gamma_ * mean\n",
    "            \n",
    "        return weight, bias\n",
    "    \n",
    "    def freeze(self, q_in=None, q_out=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'q_out') and q_out is not None:\n",
    "            raise ValueError('q_out has been provided in init function.')\n",
    "        if not hasattr(self, 'q_out') and q_out is None:\n",
    "            raise ValueError('q_out is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "        if q_out is not None:\n",
    "            self.q_out = q_out\n",
    "        \n",
    "        # 计算 M = s_w * s_in / s_out\n",
    "        self.M = self.q_weight.scale * self.q_in.scale / self.q_out.scale\n",
    "        \n",
    "        new_weight, new_bias = self.fold_bn(self.bn_module.running_mean, self.bn_module.running_var)\n",
    "        # 量化卷积层中的权重\n",
    "        self.conv_module.weight.data = self.q_weight.quantize_tensor(new_weight.data) \\\n",
    "                                        - self.q_weight.zero_point\n",
    "        \n",
    "        # 量化卷积层中的偏置\n",
    "        self.conv_module.bias.data = quantize_tensor(new_bias.data, scale=self.q_in.scale * self.q_weight.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "            \n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.q_in.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_()\n",
    "        x = x + self.q_out.zero_point\n",
    "        x.clamp_(0., 2. ** self.num_bits - 1.).round_()\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "enabling-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(64,1,28,28) *100\n",
    "\n",
    "# conv = nn.Conv2d(1, 40, 3, 1)\n",
    "# qconv1 = QConv2d(conv, has_qin=True, has_qout=True, num_bits=8)\n",
    "\n",
    "# y = qconv1(x)\n",
    "# qconv1.freeze()\n",
    "\n",
    "# qx = qconv1.q_in.quantize_tensor(x)\n",
    "# qy = qconv1.quantize_inference(qx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-level",
   "metadata": {},
   "source": [
    "### 1、定义基本模型（无bn）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "micro-heavy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 40, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(40, 40, 3, 1, groups=20)\n",
    "        self.fc = nn.Linear(5*5*40, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5*5*40)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def quantize(self, num_bits=8):\n",
    "        # 逐个量化每个模块\n",
    "        self.qconv1 = QConv2d(self.conv1, has_qin=True, has_qout=True, num_bits=num_bits)\n",
    "        self.qrelu1 = QReLU()\n",
    "        self.qmaxpool2d_1 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qconv2 = QConv2d(self.conv2, has_qin=False, has_qout=True, num_bits=num_bits)\n",
    "        self.qrelu2 = QReLU()\n",
    "        self.qmaxpool2d_2 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qfc = QLinear(self.fc, has_qin=False, has_qout=True, num_bits=num_bits)\n",
    "\n",
    "    def quantize_forward(self, x):\n",
    "        # 统计 min、max，同时模拟量化误差\n",
    "        x = self.qconv1(x)\n",
    "        x = self.qrelu1(x)\n",
    "        x = self.qmaxpool2d_1(x)\n",
    "        x = self.qconv2(x)\n",
    "        x = self.qrelu2(x)\n",
    "        x = self.qmaxpool2d_2(x)\n",
    "        x = x.view(-1, 5*5*40)\n",
    "        x = self.qfc(x)\n",
    "        return x\n",
    "\n",
    "    def freeze(self):\n",
    "        # 在统计 min、max后，对一些变量进行固化\n",
    "        self.qconv1.freeze()\n",
    "        self.qrelu1.freeze(self.qconv1.q_out)\n",
    "        self.qmaxpool2d_1.freeze(self.qconv1.q_out)\n",
    "        self.qconv2.freeze(q_in=self.qconv1.q_out)\n",
    "        self.qrelu2.freeze(self.qconv2.q_out)\n",
    "        self.qmaxpool2d_2.freeze(self.qconv2.q_out)\n",
    "        self.qfc.freeze(q_in=self.qconv2.q_out)\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        # 量化推理使用的函数\n",
    "        qx = self.qconv1.q_in.quantize_tensor(x)  # 输入量化到int8\n",
    "        qx = self.qconv1.quantize_inference(qx)\n",
    "        qx = self.qrelu1.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_1.quantize_inference(qx)\n",
    "        qx = self.qconv2.quantize_inference(qx)\n",
    "        qx = self.qrelu2.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_2.quantize_inference(qx)\n",
    "        qx = qx.view(-1, 5*5*40)\n",
    "        qx = self.qfc.quantize_inference(qx)\n",
    "        out = self.qfc.q_out.dequantize_tensor(qx)  # 输出反量化到 float\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "virtual-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(NetBN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 40, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(40)\n",
    "        self.conv2 = nn.Conv2d(40, 40, 3, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(40)\n",
    "        self.fc = nn.Linear(5 * 5 * 40, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 5 * 5 * 40)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def quantize(self, num_bits=8):\n",
    "        self.qconv1 = QConvBNReLU(self.conv1, self.bn1, has_qin=True, has_qout=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_1 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qconv2 = QConvBNReLU(self.conv2, self.bn2, has_qin=False, has_qout=True, num_bits=num_bits)\n",
    "        self.qmaxpool2d_2 = QMaxPooling2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.qfc = QLinear(self.fc, has_qin=False, has_qout=True, num_bits=num_bits)\n",
    "\n",
    "    def quantize_forward(self, x):\n",
    "        x = self.qconv1(x)\n",
    "        x = self.qmaxpool2d_1(x)\n",
    "        x = self.qconv2(x)\n",
    "        x = self.qmaxpool2d_2(x)\n",
    "        x = x.view(-1, 5*5*40)\n",
    "        x = self.qfc(x)\n",
    "        return x\n",
    "\n",
    "    def freeze(self):\n",
    "        self.qconv1.freeze()\n",
    "        self.qmaxpool2d_1.freeze(self.qconv1.q_out)\n",
    "        self.qconv2.freeze(q_in=self.qconv1.q_out)\n",
    "        self.qmaxpool2d_2.freeze(self.qconv2.q_out)\n",
    "        self.qfc.freeze(q_in=self.qconv2.q_out)\n",
    "\n",
    "    def quantize_inference(self, x):\n",
    "        qx = self.qconv1.q_in.quantize_tensor(x)\n",
    "        qx = self.qconv1.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_1.quantize_inference(qx)\n",
    "        qx = self.qconv2.quantize_inference(qx)\n",
    "        qx = self.qmaxpool2d_2.quantize_inference(qx)\n",
    "        qx = qx.view(-1, 5*5*40)\n",
    "\n",
    "        qx = self.qfc.quantize_inference(qx)\n",
    "        \n",
    "        out = self.qfc.q_out.dequantize_tensor(qx)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "suspended-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "uniform-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (datas, targets) in enumerate(train_loader):\n",
    "        datas, targets = datas.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(datas)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch:{} [{}/{}] \\t Loss: {:.4f}'.format(\n",
    "                epoch, batch_idx * len(datas), len(train_loader.dataset), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "defined-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (datas, targets) in enumerate(test_loader):\n",
    "        datas, targets = datas.to(device), targets.to(device)\n",
    "        outputs = model(datas)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {:.0f}%\\n'.format(\n",
    "        test_loss, 100. * correct / len(test_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "medieval-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_loader(batch_size, test_batch_size):\n",
    "    train_transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "    test_transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "    \n",
    "    train_dataset = datasets.MNIST('/home/xia/Dataset', train=True, download=True,\n",
    "                                  transform=train_transform)\n",
    "    test_dataset = datasets.MNIST('/home/xia/Dataset', train=False, download=True,\n",
    "                                 transform=test_transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=1)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n",
    "                                             shuffle=False, num_workers=1)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sustained-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_quantize(model, test_loader):\n",
    "    for idx ,(datas, targets) in enumerate(test_loader,1):\n",
    "        output = model.quantize_forward(datas)\n",
    "        if idx % 500 == 0:\n",
    "            break\n",
    "    print('direct quantization finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "resistant-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_inference(model, test_loader):\n",
    "    correct = 0\n",
    "    for idx, (datas, targets) in enumerate(test_loader, 1):\n",
    "        output = model(datas)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "    print('\\nTest set: Full Model Accuracy: {:.0f}%\\n'.format(100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "union-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_inference(model, test_loader):\n",
    "    correct = 0\n",
    "    for i, (datas, targets) in enumerate(test_loader, 1):\n",
    "        output = model.quantize_inference(datas)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "    print('\\nTest set: Quant Model Accuracy: {:.0f}%\\n'.format(100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "premier-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    batch_size = 64\n",
    "    test_batch_size = 64\n",
    "    using_bn = False\n",
    "    \n",
    "    train_loader, test_loader = dataset_loader(batch_size, test_batch_size)\n",
    "    \n",
    "    if using_bn:\n",
    "        model = NetBN()\n",
    "        model.load_state_dict(torch.load('ckpt/mnist_cnnbn.pt'))\n",
    "    else:\n",
    "        model = Net()\n",
    "        model.load_state_dict(torch.load('ckpt/mnist_cnn.pt'))\n",
    "    \n",
    "    model.eval()\n",
    "    full_inference(model, test_loader)\n",
    "\n",
    "    num_bits = 8\n",
    "    model.quantize(num_bits=num_bits)\n",
    "    model.eval()\n",
    "    print('Quantization bit: %d' % num_bits)\n",
    "\n",
    "    direct_quantize(model, train_loader)\n",
    "\n",
    "    model.freeze()\n",
    "\n",
    "    quantize_inference(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "little-clock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Full Model Accuracy: 98%\n",
      "\n",
      "Quantization bit: 8\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-d14b38365e9b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Quantization bit: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_bits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdirect_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-6ec93f1ee3f0>\u001b[0m in \u001b[0;36mdirect_quantize\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdirect_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-f3a099ec914e>\u001b[0m in \u001b[0;36mquantize_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqmaxpool2d_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/20210222_TensorRT官方开源库/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-bb527324e416>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfake_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         x = F.linear(input = x, \n\u001b[1;32m     15\u001b[0m                      \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfake_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-78b18c038247>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcu_max_and_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_per_channel_quantize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalcu_scale_and_zeropoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_bits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_per_channel_quantize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6bf0408505d3>\u001b[0m in \u001b[0;36mcalcu_max_and_min\u001b[0;34m(x, min_val, max_val, is_per_channel_quantize)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_val\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m  \u001b[0mmax_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mnew_max_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalcu_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mnew_max_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalcu_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-throw",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
