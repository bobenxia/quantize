{
 "cells": [
  {
   "attachments": {
    "30b79489-6c86-4382-a669-c99fa32d5667.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "binding-ready",
   "metadata": {},
   "source": [
    "## 1、后量化训练（线性量化）\n",
    "https://zhuanlan.zhihu.com/p/156835141\n",
    "\n",
    "非对称量化：\n",
    "\n",
    "![image.png](attachment:30b79489-6c86-4382-a669-c99fa32d5667.png)\n",
    "\n",
    "scale:\n",
    "> s = rmax - rmin / qmax - qmin\n",
    "\n",
    "zero point:\n",
    "> z = round(qmax - rmax / s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-eagle",
   "metadata": {},
   "source": [
    "### 1. 量化基本公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "massive-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suspended-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcu_scale_and_zeropoint(min_val, max_val, num_bits=8, is_per_channel_quantize=False):\n",
    "    # 计算 scale 和 zero_point 的基本公式\n",
    "    # is_per_channel_quantize 决定是按照逐通道量化还是按照逐层量化来计算 scale 和 zero_point \n",
    "    q_min = 0.\n",
    "    q_max = 2. ** num_bits - 1\n",
    "    \n",
    "    # 匿名函数，计算 scale 和 zero_point\n",
    "    calcu_scale =  lambda max_val, min_val : float((max_val - min_val) / (q_max - q_min))\n",
    "    calcu_zero_point = lambda max_val, scale :  np.clip(int(q_max - max_val / scale), q_min, q_max)\n",
    "    \n",
    "    # 是否是逐通道量化\n",
    "    if  is_per_channel_quantize:\n",
    "        scale = []\n",
    "        zero_point = []\n",
    "\n",
    "        for i in range(len(min_val)):\n",
    "            scale.append(calcu_scale(max_val[i], min_val[i]))\n",
    "            zero_point.append( calcu_zero_point(max_val[i], scale[i]))  \n",
    "            \n",
    "    else:\n",
    "        scale = calcu_scale(max_val, min_val)\n",
    "        zero_point = calcu_zero_point(max_val, scale)\n",
    "        \n",
    "    return scale, zero_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "major-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor(x, scale, zero_point, num_bits=8, signed=False, is_per_channel_quantize=False ):\n",
    "    # 计算 scale 和 zero_point 的基本公式\n",
    "    # is_per_channel_quantize 决定是按照逐通道量化还是按照逐层量化\n",
    "    if signed:\n",
    "        q_min = - 2. ** (num_bits - 1)\n",
    "        q_max = 2. ** (num_bits - 1) - 1.\n",
    "    else:\n",
    "        q_min = 0.\n",
    "        q_max = 2. ** num_bits -1.\n",
    "    \n",
    "    if is_per_channel_quantize:\n",
    "        q_x = torch.zeros(x.shape)\n",
    "        for i in range(len(scale)):\n",
    "            q_x[i,:,:,:] = x[i,:,:,:] /scale[i] + zero_point[i]\n",
    "            q_x[i,:,:,:].clamp_(0, 255).round()\n",
    "            \n",
    "    else:\n",
    "        q_x = x / scale + zero_point\n",
    "        q_x.clamp_(q_min, q_max).round()  # q=round(r/S+Z)\n",
    "        \n",
    "    return q_x\n",
    "\n",
    "def dequantize_tensor(q_x, scale, zero_point, is_per_channel_quantize=False ):\n",
    "    \n",
    "    if is_per_channel_quantize:\n",
    "        x = torch.zeros(q_x.shape)\n",
    "        for i in range(len(scale)):\n",
    "            x[i,:,:,:] = scale[i] * (q_x[i,:,:,:] - zero_point[i])\n",
    "            \n",
    "    else:\n",
    "        x = scale * (q_x -zero_point)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cathedral-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcu_max_and_min(x, min_val,  max_val, is_per_channel_quantize=False ):\n",
    "    # 根据最大值和最小值统计 上下限\n",
    "    calcu_max = lambda x, max_val: max(0, x.max()) if max_val is None \\\n",
    "                       else max(0, x.max(), max_val)\n",
    "    calcu_min = lambda x, min_val: min(0, x.min()) if min_val is None \\\n",
    "                       else min(0, x.min(), min_val)\n",
    "    if is_per_channel_quantize:\n",
    "#         max_val =[calcu_max(x[i,:,:,:], max_val[i]) for i in range(len(x[0]))]    \n",
    "#         min_val =[calcu_min(x[i,:,:,:], min_val[i]) for i in range(len(x[0]))]    这样写也行，不过读起来差一点\n",
    "        new_max_val = []\n",
    "        new_min_val = []\n",
    "        for i in range(len(x)):\n",
    "            if max_val == None or  max_val[i] == None:\n",
    "                new_max_val.append(calcu_max(x[i,:,:,:], None))\n",
    "            else:\n",
    "                new_max_val.append(calcu_max(x[i,:,:,:], max_val[i]))\n",
    "            if min_val == None or  min_val[i] == None:\n",
    "                new_min_val.append(calcu_min(x[i,:,:,:], None))\n",
    "            else:\n",
    "                new_min_val.append(calcu_min(x[i,:,:,:], min_val[i]))\n",
    "            \n",
    "    else:\n",
    "        new_max_val = calcu_max(x, max_val)\n",
    "        new_min_val = calcu_min(x, min_val)\n",
    "        \n",
    "    return new_min_val, new_max_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-australian",
   "metadata": {},
   "source": [
    "### 2. 封装成类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sharp-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QParam:\n",
    "    def __init__(self, num_bits=8):\n",
    "        self.num_bits = num_bits\n",
    "        self.scale = None\n",
    "        self.zero_point = None\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "    \n",
    "    def update(self, tensor, is_per_channel_quantize=False):\n",
    "        self.min, self.max = calcu_max_and_min(tensor, self.min, self.max, is_per_channel_quantize)\n",
    "        self.scale, self.zero_point = calcu_scale_and_zeropoint(self.min, self.max, self.num_bits, is_per_channel_quantize)\n",
    "        \n",
    "    def quantize_tensor(self, tensor, is_per_channel_quantize=False):\n",
    "        return quantize_tensor(tensor, self.scale, self.zero_point, self.num_bits, False, is_per_channel_quantize)\n",
    "    \n",
    "    def dequantize_tensor(self, q_x, is_per_channel_quantize=False):\n",
    "        return dequantize_tensor(q_x, self.scale, self.zero_point, is_per_channel_quantize)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-table",
   "metadata": {},
   "source": [
    "### 3、量化网络基类定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "asian-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModule(nn.Module):\n",
    "    def __init__(self, has_qin=True, has_qout=True, num_bits=8):\n",
    "        # 指定量化的位数外，还需指定是否提供量化输入 (qin) 及输出参数 (qout)\n",
    "        # 不是每一个网络模块都需要统计输入的 min、max，大部分中间层都是用上一层的 qout 来作为自己的 qin 的，\n",
    "        # 另外有些中间层的激活函数也是直接用上一层的 qin 来作为自己的 qin 和 qout。\n",
    "        super(QModule, self).__init__()\n",
    "        if has_qin:\n",
    "            self.q_in = QParam(num_bits)\n",
    "        if has_qout:\n",
    "            self.q_out = QParam(num_bits)\n",
    "    \n",
    "    def freeze(self):\n",
    "        # 函数会在统计完 min、max 后发挥作用\n",
    "        # 很多项是可以提前计算好的，freeze 就是把这些项提前固定下来\n",
    "        # 同时也将网络的权重由浮点实数转化为定点整数。\n",
    "        pass\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        # 在量化 inference 的时候会使用\n",
    "        raise NotImplementedError('quantize_inference should be implemented.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-information",
   "metadata": {},
   "source": [
    "### 4、伪量化定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "congressional-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向推导中，先量化再反量化，引入噪声\n",
    "class FakeQuantize(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, qparam, is_per_channel_quantize=False):\n",
    "        x = qparam.quantize_tensor(x, is_per_channel_quantize)\n",
    "        x = qparam.dequantize_tensor(x, is_per_channel_quantize)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None\n",
    "\n",
    "fake_quantize = FakeQuantize.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-chambers",
   "metadata": {},
   "source": [
    "### 5、量化卷积层类的定义\n",
    "<img src=\"https://www.zhihu.com/equation?tex=a%3D%5Csum_%7Bi%7D%5EN+w_i+x_i%2Bb+%5Ctag%7B1%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />\n",
    "由此得到量化的公式\n",
    "<img src=\"https://www.zhihu.com/equation?tex=S_a+%28q_a-Z_a%29%3D%5Csum_%7Bi%7D%5EN+S_w%28q_w-Z_w%29S_x%28q_x-Z_x%29%2BS_b%28q_b-Z_b%29+%5Ctag%7B2%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />\n",
    "<img src=\"https://www.zhihu.com/equation?tex=q_a%3D%5Cfrac%7BS_w+S_x%7D%7BS_a%7D%5Csum_%7Bi%7D%5EN+%28q_w-Z_w%29%28q_x-Z_x%29%2B%5Cfrac%7BS_b%7D%7BS_a%7D%28q_b-Z_b%29%2BZ_a+%5Ctag%7B3%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />\n",
    "\n",
    "\n",
    "> <img src=\"./image/image.png\" style=\"zoom:60%;\" />\n",
    "\n",
    "经过调整：\n",
    "\n",
    "<img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+q_a%26%3D%5Cfrac%7BS_w+S_x%7D%7BS_a%7D%28%5Csum_%7Bi%7D%5EN%28q_w-Z_w%29%28q_x-Z_x%29%2Bq_b%29%2BZ_a+%5Cnotag+%5C%5C+%26%3DM%28%5Csum_%7Bi%7D%5EN+q_wq_x-%5Csum_i%5EN+q_wZ_x-%5Csum_i%5EN+q_xZ_w%2B%5Csum_i%5ENZ_wZ_x%2Bq_b%29%2BZ_a+%5Ctag%7B4%7D+%5Cend%7Balign%7D+\" alt=\"[公式]\" style=\"zoom:80%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bridal-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConv2d(QModule):\n",
    "    def __init__(self, conv_module, has_qin=True, has_qout=True, num_bits=8, is_per_channel_quantize=False):\n",
    "        super(QConv2d, self).__init__(has_qin, has_qout, num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.conv_module = conv_module\n",
    "        self.q_weight = QParam(num_bits=num_bits)\n",
    "        self.weight_quantize_type = is_per_channel_quantize\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            # 更新量化输入 q_in 的参数\n",
    "            self.q_in.update(x)\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        self.q_weight.update(self.conv_module.weight.data, self.weight_quantize_type)\n",
    "        x = F.conv2d(input = x, \n",
    "                     weight = fake_quantize(self.conv_module.weight, self.q_weight, self.weight_quantize_type), \n",
    "                     bias = self.conv_module.bias, \n",
    "                     stride = self.conv_module.stride,\n",
    "                     padding = self.conv_module.padding, \n",
    "                     dilation=self.conv_module.dilation, \n",
    "                     groups = self.conv_module.groups)\n",
    "        \n",
    "        if hasattr(self, 'q_out'):\n",
    "            self.q_out.update(x)\n",
    "            x = fake_quantize(x, self.q_out)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def freeze(self, q_in=None, q_out=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'q_out') and q_out is not None:\n",
    "            raise ValueError('q_out has been provided in init function.')\n",
    "        if not hasattr(self, 'q_out') and q_out is None:\n",
    "            raise ValueError('q_out is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "        if q_out is not None:\n",
    "            self.q_out = q_out\n",
    "        \n",
    "        \n",
    "        if self.weight_quantize_type:\n",
    "            # 计算 M = s_w * s_in / s_out\n",
    "            self.M = [self.q_weight.scale[i] * self.q_in.scale / self.q_out.scale \n",
    "                      for i in range(len(self.q_weight.scale))]\n",
    "            # 计算量化后的权重\n",
    "            quantized_weight = self.q_weight.quantize_tensor(self.conv_module.weight.data, self.weight_quantize_type)\n",
    "            for i in range(len(self.q_weight.zero_point)):\n",
    "                self.conv_module.weight.data[i] = quantized_weight[i] - self.q_weight.zero_point[i]\n",
    "            # 计算量化后的偏置\n",
    "#             quantized_bias = [self.q_weight.scale[i] * self.q_in.scale for i in range(len(self.q_weight.scale))]\n",
    "#             quantized_zero_point = [0]*len(self.q_weight.scale)\n",
    "#             self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data, scale=quantized_bias,\n",
    "#                                                      zero_point=quantized_zero_point, num_bits=32, signed=True, \n",
    "#                                                          is_per_channel_quantize=self.weight_quantize_type)\n",
    "        else:\n",
    "            # 计算 M = s_w * s_in / s_out\n",
    "            self.M = self.q_weight.scale * self.q_in.scale / self.q_out.scale\n",
    "            # 计算量化后的权重\n",
    "            self.conv_module.weight.data = self.q_weight.quantize_tensor(self.conv_module.weight.data) \\\n",
    "                                        - self.q_weight.zero_point\n",
    "            # 计算量化后的偏置\n",
    "#             self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data, scale=self.q_in.scale * self.q_weight.scale,\n",
    "#                                                          zero_point=0, num_bits=32, signed=True)\n",
    "        self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data, scale=self.q_in.scale * np.mean(self.q_weight.scale),\n",
    "                                                         zero_point=0, num_bits=32, signed=True)\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.q_in.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        if self.weight_quantize_type:\n",
    "            for i in range(len(self.M)):\n",
    "                x[:,i,:,:] = x[:,i,:,:] * self.M[i]\n",
    "        else:\n",
    "            x = self.M * x\n",
    "        x.round_()\n",
    "        x = x + self.q_out.zero_point\n",
    "        x.clamp_(0., 2. ** self.num_bits - 1.).round_()\n",
    "        return x#.to(torch.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-compromise",
   "metadata": {},
   "source": [
    "### 5、量化线性层类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "detailed-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinear(QModule):\n",
    "    def __init__(self, fc_module, has_qin=True, has_qout=True, num_bits=8):\n",
    "        super(QLinear, self).__init__(has_qin, has_qout, num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.fc_module = fc_module\n",
    "        self.q_weight = QParam(num_bits=num_bits)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            x = fake_quantize(x)\n",
    "            \n",
    "        self.q_weight.update(self.fc_module.weight.data)\n",
    "        x = F.linear(input = x, \n",
    "                     weight = fake_quantize(self.fc_module.weight, self.q_weight,False), \n",
    "                     bias = self.fc_module.bias)\n",
    "        \n",
    "        if hasattr(self, 'q_out'):\n",
    "            self.q_out.update(x, is_per_channel_quantize=False)\n",
    "            x = fake_quantize(x, self.q_out,False)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def freeze(self, q_in=None, q_out=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'q_out') and q_out is not None:\n",
    "            raise ValueError('q_out has been provided in init function.')\n",
    "        if not hasattr(self, 'q_out') and q_out is None:\n",
    "            raise ValueError('q_out is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "        if q_out is not None:\n",
    "            self.q_out = q_out\n",
    "        \n",
    "        # 计算 M = s_w * s_in / s_out\n",
    "        self.M = self.q_weight.scale * self.q_in.scale / self.q_out.scale\n",
    "        \n",
    "        # 量化卷积层中的权重\n",
    "        self.fc_module.weight.data = self.q_weight.quantize_tensor(self.fc_module.weight.data) \\\n",
    "                                        - self.q_weight.zero_point\n",
    "        # 量化卷积层中的偏置\n",
    "        self.fc_module.bias.data = quantize_tensor(self.fc_module.bias.data, scale=self.q_in.scale * self.q_weight.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "        \n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.q_in.zero_point\n",
    "        x = self.fc_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_()\n",
    "        x = x + self.q_out.zero_point\n",
    "        x.clamp_(0., 2. ** self.num_bits - 1.).round_()\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-london",
   "metadata": {},
   "source": [
    "### 6、量化ReLu 类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gross-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QReLU(QModule):\n",
    "    def __init__(self, has_qin=False, num_bits=None):\n",
    "        super(QReLU, self).__init__(has_qin=has_qin,num_bits=num_bits)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def freeze(self, q_in=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        x = x.clone()\n",
    "        x[x < self.q_in.zero_point] = self.q_in.zero_point\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-amsterdam",
   "metadata": {},
   "source": [
    "### 7、量化最大池化层类的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "instant-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QMaxPooling2d(QModule):\n",
    "    def __init__(self, kernel_size=3, stride=1, padding=0, has_qin=False, num_bits=None):\n",
    "        super(QMaxPooling2d, self).__init__(has_qin=has_qin, num_bits=num_bits)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        x = F.max_pool2d(x, self.kernel_size, self.stride, self.padding)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def freeze(self, q_in=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "    \n",
    "    def quantize_inference(self, x):\n",
    "        return F.max_pool2d(x, self.kernel_size, self.stride, self.padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-google",
   "metadata": {},
   "source": [
    "### 8、卷积BNReLU合并\n",
    "卷积层的输出为：\n",
    "<img src=\"https://www.zhihu.com/equation?tex=y%3D%5Csum_%7Bi%7D%5EN+w_i+x_i+%2B+b+%5Ctag%7B1%7D+\" alt=\"[公式]\" style=\"zoom:100%;\" />\n",
    "\n",
    "BN层的输出为：\n",
    "<img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+y_%7Bbn%7D%26%3D%5Cgamma+%5Chat%7By%7D%2B%5Cbeta+%5Cnotag+%5C%5C+%26%3D%5Cgamma+%5Cfrac%7By-%5Cmu_y%7D%7B%5Csqrt%7B%5Csigma_y%5E2%2B%5Cepsilon%7D%7D%2B%5Cbeta+%5Ctag%7B2%7D+%5Cend%7Balign%7D+\" alt=\"[公式]\" style=\"zoom:100%;\" />\n",
    "\n",
    "代入：\n",
    "\n",
    "<img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+y_%7Bbn%7D%26%3D%5Cfrac%7B%5Cgamma%7D%7B%5Csqrt%7B%5Csigma_y%5E2%2B%5Cepsilon%7D%7D%28%5Csum_%7Bi%7D%5EN+w_i+x_i+%2B+b-%5Cmu_y%29%2B%5Cbeta+%5Cnotag+%5C%5C+%26%3D%5Cgamma%27%28%5Csum_%7Bi%7D%5ENw_ix_i%2Bb-%5Cmu_y%29%2B%5Cbeta+%5Cnotag+%5C%5C+%26%3D%5Csum_%7Bi%7D%5EN+%5Cgamma%27w_ix_i%2B%5Cgamma%27%28b-%5Cmu_y%29%2B%5Cbeta+%5Ctag%7B1%7D+%5Cend%7Balign%7D+\" alt=\"[公式]\" style=\"zoom:100%;\" />\n",
    "\n",
    "我们只需要用 <img src=\"https://www.zhihu.com/equation?tex=w_i%27%3D%5Cgamma%27w_i\" alt=\"[公式]\" style=\"zoom:80%;\" /> 和  <img src=\"https://www.zhihu.com/equation?tex=b%27%3D%5Cgamma%27%28b-%5Cmu_y%29%2B%5Cbeta\" alt=\"[公式]\" style=\"zoom:80%;\" />  来作为原来卷积的 weight 和 bias，就相当于把 BN 的操作合并到了 Conv 里面。实际 inference 的时候，由于 BN 层的参数已经固定了，因此可以把 BN 层 folding 到 Conv 里面，省去 BN 层的计算开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "under-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QConvBNReLU(QModule):\n",
    "    def __init__(self, conv_module, bn_module, has_qin=True, has_qout=True, num_bits=8):\n",
    "        super(QConvBNReLU, self).__init__(has_qin, has_qout, num_bits)\n",
    "        self.num_bits = num_bits\n",
    "        self.conv_module = conv_module\n",
    "        self.bn_module = bn_module\n",
    "        self.q_weight = QParam(num_bits=num_bits)\n",
    "        self.q_bias = QParam(num_bits=32)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'q_in'):\n",
    "            self.q_in.update(x)\n",
    "            x = fake_quantize(x, self.q_in)\n",
    "            \n",
    "        if self.training:  # BN层训练，更新\n",
    "            y = F.conv2d(input = x, \n",
    "                     weight = self.conv_module.weight, \n",
    "                     bias = self.conv_module.bias, \n",
    "                     stride = self.conv_module.stride,\n",
    "                     padding = self.conv_module.padding, \n",
    "                     dilation=self.conv_module.dilation, \n",
    "                     groups = self.conv_module.groups)\n",
    "            # 改变维度，为了方便channel wise计算均值和方差\n",
    "            y = y.permute(1, 0, 2, 3)  # NCHW -> CNHW\n",
    "            y = y.contiguous().view(self.conv_module.out_channels, -1)  # CNHW -> C, NHW，\n",
    "            # 计算通道维上的均值和方差\n",
    "            mean = y.mean(1)\n",
    "            var = y.var(1)\n",
    "            # 通过移动平均更新整个数据集样本的均值和方差\n",
    "            self.bn_module.running_mean =  self.bn_module.momentum * self.bn_module.running_mean + \\\n",
    "                                                                                                                    (1 - self.bn_module.momentum) * mean\n",
    "            self.bn_module.running_var = self.bn_module.momentum * self.bn_module.running_var + \\\n",
    "                                                                                                                     (1 - self.bn_module.momentum) * var\n",
    "        else:  # BN层不训练\n",
    "            mean = self.bn_module.running_mean\n",
    "            var = self.bn_module.running_var\n",
    "            \n",
    "        #  标准差 std\n",
    "        std = torch.sqrt(var, self.bn_module.eps)\n",
    "        \n",
    "        # 按照上述公式，进行合并操作，获取新的合并后的权值和偏置\n",
    "        new_weight, new_bias = self.fold_bn(mean, std)\n",
    "        self.q_weight.update(new_weight.data)\n",
    "        \n",
    "        x = F.conv2d(input = x, \n",
    "                     weight = fake_quantize(new_weight, self.q_weight), \n",
    "                     bias = new_bias, \n",
    "                     stride = self.conv_module.stride,\n",
    "                     padding = self.conv_module.padding, \n",
    "                     dilation=self.conv_module.dilation, \n",
    "                     groups = self.conv_module.groups)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        if hasattr(self, 'q_out'):\n",
    "            self.q_out.update(x)\n",
    "            x = fake_quantize(x, self.q_out)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def fold_bn(self, mean, std):\n",
    "        if self.bn_module.affine:\n",
    "            gamma_ = self.bn_module.weight / std\n",
    "            weight = self.conv_module.weight * gamma_.view(self.conv_module.out_channels, 1, 1, 1)\n",
    "            if self.conv_modules.bias is not None:\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean + self.bn_module.bias\n",
    "            else:\n",
    "                bias = self.bn_module.bias - gamma_ * mean\n",
    "        else:\n",
    "            gamma_ = 1 / std\n",
    "            weight = self.conv_module.weight * gamma_\n",
    "            if self.conv_module.bias is not None:\n",
    "                bias = gamma_ * self.conv_module.bias - gamma_ * mean\n",
    "            else:\n",
    "                bias = -gamma_ * mean\n",
    "            \n",
    "        return weight, bias\n",
    "    \n",
    "    def freeze(self, q_in=None, q_out=None):\n",
    "        if hasattr(self, 'q_in') and q_in is not None:\n",
    "            raise ValueError('q_in has been provided in init function.')\n",
    "        if not hasattr(self, 'q_in') and q_in is None:\n",
    "            raise ValueError('q_in is not existed, should be provided.')\n",
    "\n",
    "        if hasattr(self, 'q_out') and q_out is not None:\n",
    "            raise ValueError('q_out has been provided in init function.')\n",
    "        if not hasattr(self, 'q_out') and q_out is None:\n",
    "            raise ValueError('q_out is not existed, should be provided.')\n",
    "            \n",
    "        if q_in is not None:\n",
    "            self.q_in = q_in\n",
    "        if q_out is not None:\n",
    "            self.q_out = q_out\n",
    "        \n",
    "        # 计算 M = s_w * s_in / s_out\n",
    "        self.M = self.q_weight.scale * self.q_in.scale / self.q_out.scale\n",
    "        \n",
    "        new_weight, new_bias = self.fold_bn(self.bn_module.running_mean, self.bn_module.running_var)\n",
    "        # 量化卷积层中的权重\n",
    "        self.conv_module.weight.data = self.q_weight.quantize_tensor(new_weight.data) \\\n",
    "                                        - self.q_weight.zero_point\n",
    "        \n",
    "        # 量化卷积层中的偏置\n",
    "        self.conv_module.bias.data = quantize_tensor(new_bias.data, scale=self.q_in.scale * self.q_weight.scale,\n",
    "                                                     zero_point=0, num_bits=32, signed=True)\n",
    "            \n",
    "    def quantize_inference(self, x):\n",
    "        x = x - self.q_in.zero_point\n",
    "        x = self.conv_module(x)\n",
    "        x = self.M * x\n",
    "        x.round_()\n",
    "        x = x + self.q_out.zero_point\n",
    "        x.clamp_(0., 2. ** self.num_bits - 1.).round_()\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "threatened-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(64,1,28,28) *100\n",
    "\n",
    "# conv = nn.Conv2d(1, 40, 3, 1)\n",
    "# qconv1 = QConv2d(conv, has_qin=True, has_qout=True, num_bits=8)\n",
    "\n",
    "# y = qconv1(x)\n",
    "# qconv1.freeze()\n",
    "\n",
    "# qx = qconv1.q_in.quantize_tensor(x)\n",
    "# qy = qconv1.quantize_inference(qx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-seeker",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
